{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a764389-9ce9-494d-b16e-8a21422b24d4",
   "metadata": {},
   "source": [
    "# **Local RAG (Walk-through)**\n",
    "<center> \n",
    "<img src=\"../readme_images/my_rag_chart.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc3f3d-c95c-4eca-aaeb-053c995181d5",
   "metadata": {},
   "source": [
    "### **1. Create the Embeddings of the documents:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ceccf61-2924-4dcf-b00a-ccc413531248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary modules & setup global variables\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import fitz # for pdf reading\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "import spacy # For Text preprocessing \n",
    "from spacy.lang.en import English\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "from spacy_cleaner.processing import removers\n",
    "from sentence_transformers import util, SentenceTransformer # For Loading LLMs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TextIteratorStreamer # For streaming response\n",
    "import pandas as pd\n",
    "from time import perf_counter as timer\n",
    "import re\n",
    "import torch \n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import utils \n",
    "import gradio as gr\n",
    "\n",
    "# set the path to your data directory\n",
    "DATA_DIR = '../data/'\n",
    "# Define split size to turn groups of sentences into chunks\n",
    "CHUNK_SIZE_IN_SENTENCES = 6\n",
    "# define the min number of tokens in a chunk (the rest will be filtered)\n",
    "MIN_TOKEN_LENGTH_PER_CHUNK = 30\n",
    "# embedding model\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "# device\n",
    "DEVICE = \"cuda\"\n",
    "# Embedding output path\n",
    "EMBEDDING_OUTPUT_PATH = \"../vector_store/embeddings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d821e-96c6-44ff-bcd3-cdd6074bdb60",
   "metadata": {},
   "source": [
    "#### **load & preprocess documents:**\n",
    "We first load the PDF files then do some text cleaning (ex.removing URLs), and then we extract sentences. Finally, we get a list of dictionaries with the processed text and the metadata of each page of our PDFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c6dd47-05cd-4561-82a5-3d3c3e9a3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing files in directory: ../data/   ...\n",
      "processing is finished, time needed: 5.60754 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file_path': '../data\\\\attention is all you need.pdf',\n",
       " 'page_number': 10,\n",
       " 'page_char_count': 3410,\n",
       " 'page_word_count': 624,\n",
       " 'page_sentence_count': 62,\n",
       " 'page_token_count': 852.5,\n",
       " 'text': '[ 5 ] Kyunghyun Cho , Bart van Merrienboer , Caglar Gulcehre , Fethi Bougares , Holger Schwenk , and Yoshua Bengio . Learning phrase representations using rnn encoder - decoder for statistical machine translation . CoRR , abs/1406.1078 , 2014 . [ 6 ] Francois Chollet . Xception : Deep learning with depthwise separable convolutions . arXiv preprint arXiv:1610.02357 , 2016 . [ 7 ] Junyoung Chung , Çaglar Gülçehre , Kyunghyun Cho , and Yoshua Bengio . Empirical evaluation of gated recurrent neural networks on sequence modeling . CoRR , abs/1412.3555 , 2014 . [ 8 ] Chris Dyer , Adhiguna Kuncoro , Miguel Ballesteros , and Noah A. Smith . Recurrent neural network grammars . In Proc . of NAACL , 2016 . [ 9 ] Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N. Dauphin . Convolu- tional sequence to sequence learning . arXiv preprint arXiv:1705.03122v2 , 2017 . [ 10 ] Alex Graves . Generating sequences with recurrent neural networks . arXiv preprint arXiv:1308.0850 , 2013 . [ 11 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for im- age recognition . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778 , 2016 . [ 12 ] Sepp Hochreiter , Yoshua Bengio , Paolo Frasconi , and Jürgen Schmidhuber . Gradient flow in recurrent nets : the difficulty of learning long - term dependencies , 2001 . [ 13 ] Sepp Hochreiter and Jürgen Schmidhuber . Long short - term memory . Neural computation , 9(8):1735–1780 , 1997 . [ 14 ] Zhongqiang Huang and Mary Harper . Self - training PCFG grammars with latent annotations across languages . In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832–841 . ACL , August 2009 . [ 15 ] Rafal Jozefowicz , Oriol Vinyals , Mike Schuster , Noam Shazeer , and Yonghui Wu . Exploring the limits of language modeling . arXiv preprint arXiv:1602.02410 , 2016 . [ 16 ] Łukasz Kaiser and Samy Bengio . Can active memory replace attention ? In Advances in Neural Information Processing Systems , ( NIPS ) , 2016 . [ 17 ] Łukasz Kaiser and Ilya Sutskever . Neural GPUs learn algorithms . In International Conference on Learning Representations ( ICLR ) , 2016 . [ 18 ] Nal Kalchbrenner , Lasse Espeholt , Karen Simonyan , Aaron van den Oord , Alex Graves , and Ko- ray Kavukcuoglu . Neural machine translation in linear time . arXiv preprint arXiv:1610.10099v2 , 2017 . [ 19 ] Yoon Kim , Carl Denton , Luong Hoang , and Alexander M. Rush . Structured attention networks . In International Conference on Learning Representations , 2017 . [ 20 ] Diederik Kingma and Jimmy Ba . Adam : A method for stochastic optimization . In ICLR , 2015 . [ 21 ] Oleksii Kuchaiev and Boris Ginsburg . Factorization tricks for LSTM networks . arXiv preprint arXiv:1703.10722 , 2017 . [ 22 ] Zhouhan Lin , Minwei Feng , Cicero Nogueira dos Santos , Mo Yu , Bing Xiang , Bowen Zhou , and Yoshua Bengio . A structured self - attentive sentence embedding . arXiv preprint arXiv:1703.03130 , 2017 . [ 23 ] Minh - Thang Luong , Quoc V. Le , Ilya Sutskever , Oriol Vinyals , and Lukasz Kaiser . Multi - task sequence to sequence learning . arXiv preprint arXiv:1511.06114 , 2015 . [ 24 ] Minh - Thang Luong , Hieu Pham , and Christopher D Manning . Effective approaches to attention- based neural machine translation . arXiv preprint arXiv:1508.04025 , 2015 . 11',\n",
       " 'sentences': ['[ 5 ] Kyunghyun Cho , Bart van Merrienboer , Caglar Gulcehre , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .',\n",
       "  'Learning phrase representations using rnn encoder - decoder for statistical machine translation .',\n",
       "  'CoRR , abs/1406.1078 , 2014 . [',\n",
       "  '6 ] Francois Chollet .',\n",
       "  'Xception : Deep learning with depthwise separable convolutions .',\n",
       "  'arXiv preprint arXiv:1610.02357 , 2016 . [',\n",
       "  '7 ] Junyoung Chung , Çaglar Gülçehre , Kyunghyun Cho , and Yoshua Bengio .',\n",
       "  'Empirical evaluation of gated recurrent neural networks on sequence modeling .',\n",
       "  'CoRR , abs/1412.3555 , 2014 . [',\n",
       "  '8 ] Chris Dyer , Adhiguna Kuncoro , Miguel Ballesteros , and Noah A. Smith .',\n",
       "  'Recurrent neural network grammars .',\n",
       "  'In Proc .',\n",
       "  'of NAACL , 2016 . [',\n",
       "  '9 ] Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N. Dauphin .',\n",
       "  'Convolu- tional sequence to sequence learning .',\n",
       "  'arXiv preprint arXiv:1705.03122v2 , 2017 . [',\n",
       "  '10 ] Alex Graves .',\n",
       "  'Generating sequences with recurrent neural networks .',\n",
       "  'arXiv preprint arXiv:1308.0850 , 2013 . [',\n",
       "  '11 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun .',\n",
       "  'Deep residual learning for im- age recognition .',\n",
       "  'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778 , 2016 . [',\n",
       "  '12 ] Sepp Hochreiter , Yoshua Bengio , Paolo Frasconi , and Jürgen Schmidhuber .',\n",
       "  'Gradient flow in recurrent nets : the difficulty of learning long - term dependencies , 2001 . [',\n",
       "  '13 ] Sepp Hochreiter and Jürgen Schmidhuber .',\n",
       "  'Long short - term memory .',\n",
       "  'Neural computation , 9(8):1735–1780 , 1997 . [',\n",
       "  '14 ] Zhongqiang Huang and Mary Harper .',\n",
       "  'Self - training PCFG grammars with latent annotations across languages .',\n",
       "  'In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832–841 .',\n",
       "  'ACL , August 2009 . [',\n",
       "  '15 ] Rafal Jozefowicz , Oriol Vinyals , Mike Schuster , Noam Shazeer , and Yonghui Wu .',\n",
       "  'Exploring the limits of language modeling .',\n",
       "  'arXiv preprint arXiv:1602.02410 , 2016 . [',\n",
       "  '16 ] Łukasz Kaiser and Samy Bengio .',\n",
       "  'Can active memory replace attention ?',\n",
       "  'In Advances in Neural Information Processing Systems , ( NIPS ) , 2016 . [',\n",
       "  '17 ] Łukasz Kaiser and Ilya Sutskever .',\n",
       "  'Neural GPUs learn algorithms .',\n",
       "  'In International Conference on Learning Representations ( ICLR ) , 2016 . [',\n",
       "  '18 ] Nal Kalchbrenner , Lasse Espeholt , Karen Simonyan , Aaron van den Oord , Alex Graves , and Ko- ray Kavukcuoglu .',\n",
       "  'Neural machine translation in linear time .',\n",
       "  'arXiv preprint arXiv:1610.10099v2 , 2017 . [',\n",
       "  '19 ] Yoon Kim , Carl Denton , Luong Hoang , and Alexander M. Rush .',\n",
       "  'Structured attention networks .',\n",
       "  'In International Conference on Learning Representations , 2017 . [',\n",
       "  '20 ] Diederik Kingma and Jimmy Ba .',\n",
       "  'Adam : A method for stochastic optimization .',\n",
       "  'In ICLR , 2015 . [',\n",
       "  '21 ] Oleksii Kuchaiev and Boris Ginsburg .',\n",
       "  'Factorization tricks for LSTM networks .',\n",
       "  'arXiv preprint arXiv:1703.10722 , 2017 . [',\n",
       "  '22 ] Zhouhan Lin , Minwei Feng , Cicero Nogueira dos Santos , Mo Yu , Bing Xiang , Bowen Zhou , and Yoshua Bengio .',\n",
       "  'A structured self - attentive sentence embedding .',\n",
       "  'arXiv preprint arXiv:1703.03130 , 2017 . [',\n",
       "  '23 ] Minh - Thang Luong , Quoc V. Le , Ilya Sutskever , Oriol Vinyals , and Lukasz Kaiser .',\n",
       "  'Multi - task sequence to sequence learning .',\n",
       "  'arXiv preprint arXiv:1511.06114 , 2015 . [',\n",
       "  '24 ] Minh - Thang Luong , Hieu Pham , and Christopher D Manning .',\n",
       "  'Effective approaches to attention- based neural machine translation .',\n",
       "  'arXiv preprint arXiv:1508.04025 , 2015 .',\n",
       "  '11']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a sentencizer pipeline & cleaner\n",
    "NLP = English()\n",
    "NLP.add_pipe(\"sentencizer\")\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "cleaner_pipeline = Cleaner(\n",
    "    model,\n",
    "    removers.remove_url_token,\n",
    "    removers.remove_email_token)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = cleaner_pipeline.clean(text)\n",
    "    return cleaned_text\n",
    "\n",
    "def get_sentences(txt):\n",
    "    sentences = list(NLP(txt).sents)\n",
    "    sentences = [str(sentence) for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "def read_files(data_dir):\n",
    "    # loop over your files\n",
    "    print(f\"Started processing files in directory: {data_dir}   ...\")\n",
    "    t1 = timer()\n",
    "    extracted_data = []\n",
    "    for file in glob.glob(os.path.join(data_dir, \"*.pdf\")):\n",
    "        # open the doc\n",
    "        document = fitz.open(file)\n",
    "        # process\n",
    "        for page_num, page in enumerate(document):\n",
    "            # get the raw text of each page\n",
    "            txt = page.get_text()\n",
    "            # do some cleaning\n",
    "            cleaned_text = clean_text([txt])[0]\n",
    "            sentences = get_sentences(cleaned_text)\n",
    "            entry = {\"file_path\": file,\n",
    "                     \"page_number\": page_num,\n",
    "                     \"page_char_count\": len(cleaned_text),\n",
    "                     \"page_word_count\": len(cleaned_text.split(\" \")),\n",
    "                     \"page_sentence_count\": len(sentences),\n",
    "                     \"page_token_count\": len(cleaned_text) / 4,\n",
    "                     \"text\": cleaned_text,\n",
    "                     \"sentences\": sentences}\n",
    "            extracted_data.append(entry)\n",
    "    t2 = timer()\n",
    "    print(f\"processing is finished, time needed: {t2 - t1:.5f} seconds\")\n",
    "    return extracted_data\n",
    "\n",
    "# load & process documents\n",
    "extracted_data = read_files(DATA_DIR)\n",
    "extracted_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "661ccd23-1548-410c-b0bc-675bd203e932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>79.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>79.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.29</td>\n",
       "      <td>3382.61</td>\n",
       "      <td>635.19</td>\n",
       "      <td>28.14</td>\n",
       "      <td>845.65</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.75</td>\n",
       "      <td>1042.17</td>\n",
       "      <td>193.59</td>\n",
       "      <td>15.02</td>\n",
       "      <td>260.54</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>845.00</td>\n",
       "      <td>176.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>211.25</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.50</td>\n",
       "      <td>2651.00</td>\n",
       "      <td>490.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>662.75</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.00</td>\n",
       "      <td>3607.00</td>\n",
       "      <td>677.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>901.75</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.00</td>\n",
       "      <td>3958.00</td>\n",
       "      <td>759.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>989.50</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.00</td>\n",
       "      <td>5503.00</td>\n",
       "      <td>961.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>1375.75</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count  \\\n",
       "count        79.00            79.00            79.00                79.00   \n",
       "mean         11.29          3382.61           635.19                28.14   \n",
       "std           8.75          1042.17           193.59                15.02   \n",
       "min           0.00           845.00           176.00                 8.00   \n",
       "25%           4.50          2651.00           490.00                18.00   \n",
       "50%           9.00          3607.00           677.00                24.00   \n",
       "75%          16.00          3958.00           759.00                35.00   \n",
       "max          33.00          5503.00           961.00                64.00   \n",
       "\n",
       "       page_token_count  num_chunks  \n",
       "count             79.00       79.00  \n",
       "mean             845.65        5.09  \n",
       "std              260.54        2.50  \n",
       "min              211.25        2.00  \n",
       "25%              662.75        3.00  \n",
       "50%              901.75        4.00  \n",
       "75%              989.50        6.00  \n",
       "max             1375.75       11.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets veiw some stats\n",
    "df = pd.DataFrame(extracted_data)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e6936-34e9-46fc-8fc9-a5def2f23177",
   "metadata": {},
   "source": [
    "From the stats, we can see that the average num of chunks per page is 5, and the average token count is 845. we can conclude that each chunk has 845/5 ~ 169 tokens. meaning we need to choose an embedding model that has a context length >= 169. for example all-mpnet-base-v2 model (it has a capacity of 384 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47b024-4e5b-4922-a5e3-f7c41fe288de",
   "metadata": {},
   "source": [
    "#### **Chunking The text into a group of sentences**\n",
    "We are grouping each couple of sentences into one chunk. We are also keeping their metadata like (file path, page number, etc) to be able to return citations when we generate the RAG answer. The need to split pages into smaller chunks is because the embedding model has a limited context length, in our model **all-mpnet-base-v2** is 384 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6376199-369b-4922-b3f0-db2cab52125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text ..\n",
      "Chunking is finished, time needed: 0.00767 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file_path': '../data\\\\attention is all you need.pdf',\n",
       " 'page_number': 2,\n",
       " 'sentence_chunk': 'This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors . The output is computed as a weighted sum 3',\n",
       " 'chunk_char_count': 420,\n",
       " 'chunk_word_count': 82,\n",
       " 'chunk_token_count': 105.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_chunk_dict(text_dict):\n",
    "    extracted_chunks = []\n",
    "    for item in text_dict:\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"file_path\"] = item[\"file_path\"]\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "            # Join the sentences together into a paragraph-like structure\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            # add a space after a period if it's followed by an uppercase letter\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4  # 1 token = ~4 characters\n",
    "            extracted_chunks.append(chunk_dict)\n",
    "    return extracted_chunks\n",
    "\n",
    "def chunking(list_of_sentences, chunk_size):\n",
    "    # We group sentences based on the chunk size (estimated in sentences)\n",
    "    sentence_chunks = [list_of_sentences[i:i + chunk_size] for i in range(0, len(list_of_sentences), chunk_size)]\n",
    "    return sentence_chunks\n",
    "\n",
    "def chunk_text(data, chunk_size_in_sentences):\n",
    "    print(\"Chunking text ..\")\n",
    "    t1 = timer()\n",
    "    for entry in data:\n",
    "        entry[\"sentence_chunks\"] = chunking(entry[\"sentences\"], chunk_size_in_sentences)\n",
    "        entry[\"num_chunks\"] = len(entry[\"sentence_chunks\"])\n",
    "    # here we create new dict with chunks as entries & we keep their metadata\n",
    "    extracted_chunks = convert_to_chunk_dict(data)\n",
    "    t2 = timer()\n",
    "    print(f\"Chunking is finished, time needed: {t2 - t1:.5f} seconds\")\n",
    "    return extracted_chunks\n",
    "\n",
    "\n",
    "# chunking into groups of sentences\n",
    "extracted_chunks = chunk_text(extracted_data, CHUNK_SIZE_IN_SENTENCES)\n",
    "extracted_chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a144344-04ae-42d8-a242-df03bfcf8888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>402.00</td>\n",
       "      <td>402.00</td>\n",
       "      <td>402.00</td>\n",
       "      <td>402.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.16</td>\n",
       "      <td>662.79</td>\n",
       "      <td>123.68</td>\n",
       "      <td>165.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.65</td>\n",
       "      <td>384.15</td>\n",
       "      <td>71.00</td>\n",
       "      <td>96.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.00</td>\n",
       "      <td>370.75</td>\n",
       "      <td>68.25</td>\n",
       "      <td>92.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.00</td>\n",
       "      <td>611.50</td>\n",
       "      <td>112.50</td>\n",
       "      <td>152.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.00</td>\n",
       "      <td>903.50</td>\n",
       "      <td>168.00</td>\n",
       "      <td>225.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.00</td>\n",
       "      <td>2328.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>582.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count       402.00            402.00            402.00             402.00\n",
       "mean         11.16            662.79            123.68             165.70\n",
       "std           7.65            384.15             71.00              96.04\n",
       "min           0.00              2.00              1.00               0.50\n",
       "25%           6.00            370.75             68.25              92.69\n",
       "50%          10.00            611.50            112.50             152.88\n",
       "75%          14.00            903.50            168.00             225.88\n",
       "max          33.00           2328.00            393.00             582.00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets veiw some stats\n",
    "df = pd.DataFrame(extracted_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e570f-2fba-4d53-9ed8-43d00e93c532",
   "metadata": {},
   "source": [
    "Then we filter out chunks with token count less that **min_token_length_per_chunk**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8b3b1d-77cc-4676-9fe6-fa0a0f4a4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_chunks(chunks, min_token_length_per_chunk):\n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    filtered_chunks = chunks_df[chunks_df[\"chunk_token_count\"]\n",
    "                                > min_token_length_per_chunk].to_dict(orient=\"records\")\n",
    "    return filtered_chunks\n",
    "\n",
    "\n",
    "# filter short chunks\n",
    "extracted_chunks_df_filtered = filter_chunks(extracted_chunks, MIN_TOKEN_LENGTH_PER_CHUNK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c687059-2bc8-4b2d-8944-95852fffa326",
   "metadata": {},
   "source": [
    "#### **Create the Embeddings** \n",
    "Here we load the embedding model and then we embedd each chunk of the docuemnts. the output vectors has dimension of  **768**. Then we save these embeddings to the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c3c761-46b8-463c-af68-764b33b7f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model \"all-mpnet-base-v2\" on GPU  ...\n",
      "Started creating the embeddings ...\n",
      "Creating the embeddings is finished, time needed: 6.94670 seconds\n",
      "Embeddings have been saved on disk at:  ../vector_store/embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings(chunks_df, embedding_model_name, device, embedding_output_path):\n",
    "    # load embedding model\n",
    "    print(f\"Loading embedding model \\\"{embedding_model_name}\\\" on {'GPU' if device == 'cuda' else device}  ...\")\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=embedding_model_name, device=device)\n",
    "\n",
    "    # start creating the embeddings\n",
    "    print(\"Started creating the embeddings ...\")\n",
    "    t1 = timer()\n",
    "    for item in tqdm(chunks_df):\n",
    "        item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "    t2 = timer()\n",
    "    print(f\"Creating the embeddings is finished, time needed: {t2 - t1:.5f} seconds\")\n",
    "\n",
    "    # save the embedding on disk\n",
    "    embeddings_df = pd.DataFrame(chunks_df)\n",
    "    os.makedirs(os.path.dirname(embedding_output_path), exist_ok=True)\n",
    "    embeddings_df.to_csv(embedding_output_path, index=False, escapechar=\"\\\\\")\n",
    "    print(f\"Embeddings have been saved on disk at:  {embedding_output_path}\")\n",
    "\n",
    "\n",
    "# load embedding model & create embedding & save on disk\n",
    "create_embeddings(extracted_chunks_df_filtered, EMBEDDING_MODEL,\n",
    "                 DEVICE, EMBEDDING_OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af22824-7f9f-44aa-af8c-f5d16250ecce",
   "metadata": {},
   "source": [
    "### **2. Build the RAG Pipeline:**\n",
    "We have already created a vector store that holds the locally created embeddings, now we need to have Q & A pipeline with augmented-prompts by our data files and **LLM** that is hosted **locally (on my GPU)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de84591d-59a3-4885-8dff-0e312a4ddf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_STORE_PATH = \"../vector_store/embeddings.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "NUM_OF_RELEVANT_CHUNKS = 5\n",
    "SIMILARITY_THRESHOLD = 0.3\n",
    "LLM_MODEL_ID = \"google/gemma-2b-it\"\n",
    "TEMPERATURE = 0.1\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282605e-53ee-4e77-b27e-57d1aebb437d",
   "metadata": {},
   "source": [
    "#### **Load the vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9367058f-a05e-4e5a-b0aa-f5616a8dcff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector dim: 768, device: cuda\n",
      "tensor([-5.4377e-05, -7.6402e-02, -1.7080e-02,  2.7743e-02, -4.9856e-02,\n",
      "         2.4300e-02,  5.5249e-02,  2.0908e-03, -1.6833e-02, -2.3803e-03,\n",
      "         3.8728e-03, -4.2185e-02,  3.8551e-02,  3.8432e-02,  8.0141e-02,\n",
      "        -7.0942e-02,  1.7040e-02, -2.2024e-02, -7.9899e-02, -7.3238e-03,\n",
      "        -2.3346e-02, -5.0946e-02, -1.8225e-02,  4.4284e-02,  1.2815e-02,\n",
      "        -1.7583e-02,  5.1758e-03, -1.5506e-02,  4.4298e-03, -2.7047e-02,\n",
      "        -4.8557e-03,  1.8168e-02,  2.2040e-02, -8.7429e-03,  1.8417e-06,\n",
      "        -1.2308e-03, -7.2533e-03, -5.0766e-04, -1.9028e-02, -3.4150e-02,\n",
      "        -4.7926e-02, -3.0096e-03,  2.4131e-02,  8.9170e-03, -2.9060e-03,\n",
      "         4.4912e-02,  4.0294e-02,  2.7502e-02,  1.7774e-02,  3.4721e-02,\n",
      "        -2.3266e-03, -2.1491e-02, -2.2072e-02, -4.3160e-02,  8.3654e-02,\n",
      "        -9.8258e-03,  3.9857e-02, -6.5048e-02, -6.2145e-02, -2.5086e-03,\n",
      "        -1.9408e-02,  2.5230e-02,  1.3058e-02, -1.4173e-02,  9.8689e-02,\n",
      "         4.8417e-02,  1.4975e-02, -4.2299e-02, -1.4694e-02, -4.3810e-02,\n",
      "        -4.7463e-02, -2.7358e-03, -6.3313e-03,  2.8316e-02,  2.8112e-03,\n",
      "        -5.2418e-02, -6.7661e-02, -3.4745e-03,  4.7372e-02,  1.6934e-02,\n",
      "        -7.0086e-03,  7.6919e-02, -2.2881e-02, -5.5306e-02, -1.5720e-02,\n",
      "         2.6851e-02, -3.0834e-03, -2.4928e-02, -3.8408e-02,  3.4219e-03,\n",
      "        -1.4278e-04, -6.4944e-02,  4.6251e-02,  4.1831e-02,  4.2606e-02,\n",
      "        -2.5525e-02,  6.0317e-04, -3.2298e-02, -3.1404e-02, -4.7721e-02,\n",
      "         4.7851e-02,  3.1533e-02,  1.2969e-02,  4.0762e-02, -1.6559e-02,\n",
      "         1.0428e-01, -6.0352e-02,  3.9255e-02, -1.9524e-02,  4.7520e-02,\n",
      "         5.3132e-02, -3.9037e-02, -2.1067e-02,  8.8055e-02, -3.0731e-02,\n",
      "        -3.0686e-02, -1.3525e-02,  1.8547e-02, -5.2975e-02,  4.4862e-02,\n",
      "        -1.3222e-02,  7.9869e-04, -4.9375e-02, -1.9687e-02, -9.0165e-03,\n",
      "         1.0881e-02,  2.9929e-03, -4.2870e-03,  1.4364e-05,  3.2237e-02,\n",
      "        -3.6136e-02,  2.3905e-02,  5.3715e-03, -2.2551e-02, -6.5893e-02,\n",
      "         8.7951e-02,  1.2106e-02,  1.6585e-02,  3.1874e-02,  4.0040e-03,\n",
      "         2.2974e-02, -1.0137e-02,  9.3915e-02, -9.7559e-03, -1.8313e-02,\n",
      "         3.7619e-02, -1.5084e-02, -1.2477e-03,  2.0159e-02,  2.8254e-02,\n",
      "        -2.1098e-02,  1.8925e-02,  6.5477e-02, -2.1045e-02, -2.3233e-02,\n",
      "         1.6982e-02, -1.0880e-02,  3.0401e-02, -1.7330e-02,  6.2050e-02,\n",
      "        -2.7578e-03, -2.4875e-02, -2.8664e-02, -4.2155e-02,  3.3202e-02,\n",
      "        -1.5189e-02, -2.8173e-02, -3.3032e-03,  1.9619e-02, -2.1976e-02,\n",
      "         4.2270e-02, -1.6413e-03, -1.2591e-02, -3.9076e-02,  4.7763e-02,\n",
      "         6.4502e-02,  6.8688e-02,  1.4349e-02, -4.8524e-03, -1.6349e-02,\n",
      "        -3.8704e-02,  9.2617e-02, -3.2514e-02,  8.0499e-02, -1.0954e-02,\n",
      "        -8.4013e-03, -5.9157e-03,  8.8856e-03, -3.0504e-03, -2.6372e-02,\n",
      "         3.2603e-03,  5.0098e-02, -2.2662e-02, -3.4000e-02, -1.3629e-02,\n",
      "         4.0063e-03, -4.8634e-02,  1.2321e-02, -2.0122e-02,  2.3718e-02,\n",
      "         3.3028e-02, -1.3778e-02, -2.0450e-02,  1.4464e-02,  6.2113e-03,\n",
      "         1.2823e-04,  5.3845e-03, -3.0832e-02, -3.4795e-02,  4.3626e-02,\n",
      "        -1.5023e-02,  1.9570e-02, -1.3055e-02, -1.6789e-03, -3.5835e-02,\n",
      "        -1.9188e-02, -1.9055e-02,  4.8649e-03, -5.0797e-02, -3.2652e-02,\n",
      "         4.6389e-03,  2.9140e-02,  7.2518e-03, -1.9515e-04,  3.6042e-03,\n",
      "        -3.8272e-02, -5.7892e-02,  3.3269e-02, -3.5579e-03,  1.4119e-02,\n",
      "         2.9010e-02,  3.1289e-02,  7.1062e-02, -1.5973e-02,  5.8880e-02,\n",
      "         6.8585e-03, -3.1109e-02,  6.2213e-03,  6.6743e-03, -5.5649e-02,\n",
      "         4.2665e-02,  4.5955e-02,  4.6699e-02, -3.9671e-02,  4.8459e-02,\n",
      "        -4.1761e-02, -4.6735e-03,  3.4315e-03, -1.8395e-02,  1.9101e-02,\n",
      "         9.8467e-03,  2.4099e-04,  3.7321e-02, -3.5768e-02,  2.1856e-02,\n",
      "        -1.8416e-03,  1.8430e-03,  1.1508e-02, -5.7397e-02,  3.9220e-02,\n",
      "        -1.8293e-03, -1.9323e-02, -7.9322e-03, -1.8593e-02, -5.2496e-02,\n",
      "        -1.9897e-02, -1.3339e-02,  9.5139e-03, -1.6593e-02, -8.8968e-03,\n",
      "         1.3138e-02,  3.1487e-02, -2.9212e-02,  2.8795e-02, -1.7560e-02,\n",
      "        -1.0138e-02, -7.2386e-03, -2.5762e-02, -1.4849e-03,  4.0962e-02,\n",
      "         6.6180e-03, -1.0033e-03, -1.9950e-02, -3.9127e-02, -2.5601e-03,\n",
      "         5.6038e-02,  1.5835e-02, -8.9324e-02, -5.4134e-02, -1.1238e-02,\n",
      "         1.9488e-02,  4.3083e-02, -3.3899e-03, -1.4830e-02, -1.0930e-02,\n",
      "        -3.4570e-02,  7.8691e-03,  1.0243e-01,  2.0509e-02,  6.1831e-02,\n",
      "        -3.5316e-02,  2.3535e-02, -9.9712e-03,  2.9865e-02, -6.7886e-02,\n",
      "         6.1086e-02, -3.3097e-02, -1.6505e-02, -2.3408e-02,  4.5575e-02,\n",
      "         1.3724e-02, -3.8043e-03, -7.9038e-02,  3.1723e-02,  1.8343e-02,\n",
      "        -1.1792e-02, -6.2697e-02,  4.6264e-03, -4.8250e-02,  8.8538e-03,\n",
      "        -4.3558e-02, -3.0709e-02,  1.7254e-02, -1.3319e-02, -4.6332e-03,\n",
      "         1.5057e-04,  5.6756e-02,  7.4185e-03, -6.1962e-03,  5.7066e-02,\n",
      "         2.8986e-02, -2.8702e-02, -3.7594e-02, -4.5449e-04,  3.6963e-02,\n",
      "        -3.8728e-02, -2.6995e-02, -2.1815e-02,  5.2842e-02, -2.7856e-02,\n",
      "        -2.3597e-02, -2.1944e-02, -7.2757e-03, -2.7179e-02,  3.8891e-02,\n",
      "         2.7839e-02,  2.4321e-02,  7.4908e-03, -5.5221e-02,  3.1974e-02,\n",
      "         8.6258e-02, -1.1609e-02, -4.9301e-02,  1.6939e-02, -5.0234e-02,\n",
      "        -3.1301e-05,  1.0644e-02,  4.4063e-02,  4.2200e-03, -3.2099e-02,\n",
      "        -5.4442e-03,  6.0681e-02,  3.3826e-02,  4.8913e-02,  2.6720e-02,\n",
      "         2.2305e-02,  7.0454e-02, -1.9368e-02, -2.8562e-02,  4.5002e-03,\n",
      "         5.9741e-03, -1.4647e-02, -1.8766e-02,  1.2898e-02, -1.1733e-02,\n",
      "        -4.8778e-02,  1.2457e-02, -1.8893e-02,  5.1068e-02,  6.1626e-02,\n",
      "         2.7027e-02, -2.8531e-02,  2.2967e-02, -6.6110e-03,  1.4364e-02,\n",
      "         2.4366e-02, -2.2609e-02,  3.2979e-02, -8.6018e-02,  5.4233e-02,\n",
      "         1.5968e-02, -5.9449e-02, -9.4487e-02, -1.3068e-02,  5.9432e-03,\n",
      "         4.3174e-02, -5.0015e-02,  1.9301e-02, -4.2915e-03, -1.2630e-02,\n",
      "         8.2068e-02,  3.4620e-02, -2.1885e-03, -1.1837e-02, -3.6387e-03,\n",
      "         5.6727e-03,  8.6687e-02,  1.2765e-02, -2.7769e-02, -6.9393e-02,\n",
      "        -3.0990e-02, -3.6064e-02,  1.6199e-02,  5.3859e-03,  8.7526e-02,\n",
      "        -1.1352e-02, -1.2297e-02, -5.9134e-03,  3.2611e-03,  1.8027e-02,\n",
      "        -1.0375e-03, -7.3185e-02,  5.8827e-02,  7.9218e-03, -1.9736e-02,\n",
      "        -1.6517e-02,  7.1976e-02, -9.8505e-03,  5.4434e-04,  6.9172e-02,\n",
      "         8.9277e-03,  3.4447e-03, -5.5561e-02,  9.2531e-02, -4.3235e-02,\n",
      "        -2.9894e-03,  2.3520e-02,  2.1617e-02, -2.7716e-02, -3.1645e-02,\n",
      "        -4.5289e-05,  1.3459e-02,  6.7139e-02,  2.0699e-02, -2.8335e-02,\n",
      "        -1.1071e-02, -3.6611e-02, -1.4194e-02,  1.7354e-02,  2.8483e-02,\n",
      "         2.2149e-02,  1.8557e-02,  3.4910e-03,  5.6138e-03, -6.5100e-02,\n",
      "        -4.4681e-02, -3.8822e-02, -4.6860e-02, -2.8213e-02,  3.1378e-02,\n",
      "         6.5992e-02,  3.3304e-02,  1.2260e-02, -4.6719e-02, -3.3525e-02,\n",
      "         5.8512e-03, -4.7903e-02,  4.3008e-02,  6.1922e-02,  4.2676e-02,\n",
      "        -5.8308e-02,  3.8241e-03,  8.0917e-03, -2.9363e-02, -5.4065e-03,\n",
      "         2.0456e-02, -2.3163e-02,  1.4390e-02,  2.8245e-03, -7.5919e-03,\n",
      "        -2.7993e-02,  9.6836e-02,  5.8096e-02,  4.7874e-02, -3.9114e-04,\n",
      "         7.1752e-02, -3.3161e-02, -1.0512e-02, -2.9422e-02, -8.5846e-03,\n",
      "         2.7825e-02,  3.2131e-02,  6.4194e-03,  4.3507e-02,  3.9197e-02,\n",
      "        -6.6349e-02, -4.0741e-02,  1.3274e-02, -2.3305e-02, -3.1877e-02,\n",
      "        -3.4292e-02,  3.9397e-02, -4.3549e-02,  2.1331e-02, -2.2919e-02,\n",
      "         4.0445e-02,  1.7697e-02, -3.7372e-03,  2.7460e-02, -6.2039e-03,\n",
      "        -4.3908e-02,  2.9072e-02, -8.4552e-02, -2.3997e-02, -1.0357e-02,\n",
      "        -2.5420e-02,  3.4170e-02, -2.6979e-02,  2.6574e-02,  3.0303e-02,\n",
      "        -1.9985e-02,  5.4280e-03, -1.8894e-02,  2.0719e-02, -4.5155e-03,\n",
      "         1.3765e-02, -5.4322e-03,  1.7032e-03, -5.0612e-02,  5.5431e-02,\n",
      "        -5.3681e-03, -2.1080e-02, -5.6580e-03, -2.1121e-02, -5.6428e-03,\n",
      "        -2.1036e-02,  5.4260e-02,  9.1305e-02,  3.2568e-02,  1.7002e-02,\n",
      "        -1.3550e-02, -4.5494e-02,  4.2799e-02, -6.8593e-03,  2.9657e-02,\n",
      "        -6.7744e-02,  8.2327e-03,  2.3426e-03,  1.3598e-02, -6.9033e-02,\n",
      "        -2.5190e-02,  1.2876e-02, -1.8013e-02,  2.2768e-04,  1.2762e-02,\n",
      "        -6.4958e-33, -3.2740e-02, -4.3793e-02,  5.5581e-03,  2.6072e-02,\n",
      "        -2.5234e-02,  8.8903e-04, -2.0417e-03, -5.3289e-02,  2.9879e-03,\n",
      "        -3.0271e-02, -7.8590e-03,  6.0642e-03,  1.0021e-02,  1.9934e-03,\n",
      "        -3.5318e-02, -2.6275e-02, -9.6834e-03, -2.0763e-02, -5.4656e-03,\n",
      "         1.0547e-02,  4.0971e-02,  3.4968e-02,  6.5653e-02, -3.2491e-02,\n",
      "        -1.8021e-02,  3.7412e-03, -3.7005e-02, -1.5097e-02, -5.0582e-02,\n",
      "         1.9121e-02, -5.0407e-02, -4.8781e-02, -5.1533e-03, -6.8717e-02,\n",
      "        -3.6131e-02,  2.0689e-02, -4.4328e-02, -3.5658e-02,  3.9454e-02,\n",
      "         4.3592e-03, -5.5450e-02, -2.7302e-02,  7.5915e-02,  1.2597e-02,\n",
      "        -3.3191e-02, -5.7226e-02, -3.0530e-03, -2.4014e-02, -4.9841e-02,\n",
      "         1.4870e-02, -3.4628e-02,  5.6031e-03, -1.3526e-02,  9.9071e-03,\n",
      "         4.1054e-02,  5.8278e-02,  4.3434e-02,  7.6450e-03, -7.5667e-02,\n",
      "         3.4920e-02,  1.7858e-02,  7.8320e-02, -5.6351e-03,  5.4189e-02,\n",
      "         3.7135e-02,  3.4485e-02,  6.6205e-02,  3.7515e-04, -2.1123e-02,\n",
      "        -1.7742e-02, -2.1117e-02, -1.8415e-02,  6.7738e-02, -7.7350e-02,\n",
      "         3.5372e-02, -1.2369e-02, -4.3105e-02, -1.4801e-03, -1.2971e-01,\n",
      "         3.8834e-02,  6.3263e-02,  3.0166e-02, -6.6062e-02, -2.2828e-02,\n",
      "        -7.5403e-03, -7.7208e-02, -6.6861e-03,  3.0524e-03,  8.7248e-03,\n",
      "         8.1712e-03,  2.9657e-03,  1.0772e-01, -1.1787e-03, -9.8276e-03,\n",
      "         5.9737e-02, -1.2247e-02,  1.6520e-02,  3.2709e-02, -1.5713e-03,\n",
      "        -1.0873e-02, -3.7607e-02,  4.4839e-02, -4.5317e-03, -8.7235e-03,\n",
      "         2.2941e-02,  3.2368e-02,  2.7152e-02,  4.2070e-02, -1.4857e-02,\n",
      "        -7.6534e-02, -4.5791e-02,  1.2060e-02, -6.8792e-03, -7.4145e-03,\n",
      "         3.3934e-02, -3.4279e-02, -6.9609e-04,  4.5309e-02,  3.0121e-02,\n",
      "         3.2156e-02, -2.8409e-03,  7.6773e-03,  1.6016e-03,  1.2917e-02,\n",
      "        -1.0905e-02, -1.9910e-02, -3.6482e-02,  8.0624e-02,  3.4166e-02,\n",
      "        -4.1920e-02, -4.1865e-02,  1.9733e-02,  2.7473e-07,  4.5259e-02,\n",
      "         5.8463e-03,  3.3476e-02,  3.9356e-02,  4.5733e-02,  4.2978e-03,\n",
      "         1.1530e-02,  7.6259e-02,  6.0735e-02,  8.7201e-03, -2.4405e-02,\n",
      "         3.2374e-02, -1.8704e-02, -2.0704e-02,  2.3071e-02, -5.4389e-02,\n",
      "        -2.3505e-02,  2.3348e-04,  1.0896e-03,  3.3249e-02, -3.2178e-02,\n",
      "         6.8764e-02,  2.6904e-04,  3.7186e-02,  1.7651e-02, -1.4638e-02,\n",
      "         4.5266e-02,  1.1054e-02,  5.3387e-02, -4.4203e-02,  7.5250e-02,\n",
      "        -1.1843e-02, -3.4108e-02, -7.8693e-03, -2.4118e-02,  3.0884e-02,\n",
      "         2.8299e-02,  5.4709e-02, -3.8302e-02, -3.1285e-02, -1.7980e-02,\n",
      "        -1.6783e-02,  1.5070e-02, -8.1948e-02,  1.6721e-02, -3.6629e-02,\n",
      "        -2.2235e-02, -1.0309e-02, -1.4892e-05,  3.5861e-02, -3.0957e-02,\n",
      "         6.3924e-03, -3.7017e-02, -4.8979e-02,  1.3778e-02, -3.7949e-02,\n",
      "         2.3599e-03, -4.1103e-02, -3.1503e-02, -3.3197e-03, -5.2105e-02,\n",
      "        -8.7641e-03, -1.5459e-02, -5.3716e-03, -9.2200e-04, -3.5898e-02,\n",
      "        -5.9379e-02,  2.6901e-34, -1.9734e-02, -1.7870e-02,  2.9269e-03,\n",
      "        -1.6592e-02,  2.5772e-02, -6.5965e-02,  1.8223e-02,  8.8014e-03,\n",
      "         3.6669e-02, -3.2662e-02, -7.4009e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def load_vector_store(vector_store_path, device):\n",
    "    loaded_df = pd.read_csv(vector_store_path)\n",
    "    # Convert embedding column back to np.array if they were string\n",
    "    if isinstance(loaded_df[\"embedding\"][0], str):\n",
    "        loaded_df[\"embedding\"] = loaded_df[\"embedding\"].apply(\n",
    "            lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "    # Convert texts and embedding df to list of dicts (data index)\n",
    "    data_index = loaded_df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Convert embeddings to torch tensor and send to device\n",
    "    embeddings = torch.tensor(np.array(loaded_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "    return embeddings, data_index\n",
    "\n",
    "# load the vector-store\n",
    "embeddings, data_index = load_vector_store(VECTOR_STORE_PATH, DEVICE)\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL, device=DEVICE)\n",
    "\n",
    "# show samples\n",
    "print(f\"vector dim: {len(embeddings[10])}, device: {DEVICE}\")\n",
    "print(embeddings[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b5112-dc0d-45d6-9be9-4d670c42cdc7",
   "metadata": {},
   "source": [
    "#### **Load LLM in 4bit Precision**\n",
    "Here we load **\"google/gemma-2b-it\"** model which has 2 billion parameters. My GPU is **Nvidia RTX 3060** with **6GB** memory. Loading 2 Billion  parameters model in full precision needs **2b * 4 ~ 8GB** of GPU memory. I need to do quantization to **int-8** or **int-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "472a12a9-c0fc-4d0e-a3b5-18ec54043916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b6a5e8f990447490e7b01dcb5da6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_llm(model_id):\n",
    "    # load in 4bit precision (boost the inference time significantly)\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                     torch_dtype=torch.bfloat16,\n",
    "                                                     quantization_config=quantization_config,\n",
    "                                                     low_cpu_mem_usage=False)\n",
    "    return tokenizer, llm_model\n",
    "\n",
    "# Load LLM locally \"google/gemma-2b-it\"\n",
    "tokenizer, llm_model = load_llm(model_id=LLM_MODEL_ID)\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffabba0-2c3b-4001-895f-936d310c90fa",
   "metadata": {},
   "source": [
    "from this, we can notice:\n",
    "- Vocab size is **256k**\n",
    "- The hidden size is **2048**\n",
    "- Context length from the model card **8192**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcb02d-6412-412e-baa7-89fc404791c4",
   "metadata": {},
   "source": [
    "#### **Build the Retriever**\n",
    "We calculate then **dot product** between the **embedded query** and the **embeddings** in our vector store. The **dot product** is the same as the **cosine similarity** when the vectors are **normalised**, which is the case of the output of our embedding model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6405658e-633b-4d5b-800d-a9252e3011cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: VPT video pre-training\n",
      "\n",
      "Results:\n",
      "\n",
      "Score: 0.4666\n",
      "File path: ../data\\\\video pretraining VPT.pdf\n",
      "Page number: 3\n",
      "Text:\n",
      "Collecting “ Clean ” Data Training the VPT Foundation Model via Behavioral Cloning Training the Inverse Dynamics Model ( IDM ) ~270k hours unlabeled video ~70k hours unlabeled video ~2k hours video labeled with actions Filter for “ clean ” video segments Search for relevant Minecraft videos via keywords Contractors collect data Label videos with IDM ~70k hours video IDM - labeled with actions Train non - causal IDM Train causal VPT Foundation Model a d space w a d space w Figure 2 : Video Pretraining ( VPT ) Method Overview .3 Methods Inverse Dynamics Models ( IDM ) VPT , illustrated in Figure 2 , requires we first collect a small amount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1 ... T ) , which seeks to minimize the negative log - likelihood of an action at timestep t given a trajectory of T observations ot : t ∈ [ 1 ... T ] . In contrast to an imitation learning policy , the IDM can be non - causal , meaning its prediction for at can be a function of both past and future events , i.e. ot′>t . Compared to the behavioral cloning objective of modeling the distribution of human intent given past frames only , we hypothesize that inverting environment dynamics is easier and more data efficient to learn . Indeed , Sec .4.1 will show that the IDM objective is much easier to learn , and furthermore Sec .\n",
      "\n",
      "\n",
      "Score: 0.4581\n",
      "File path: ../data\\\\video pretraining VPT.pdf\n",
      "Page number: 8\n",
      "Text:\n",
      "Compared to generative video modeling or contrastive methods that would only yield representational priors , VPT offers the exciting possibility of directly learning to act during pretraining and using these learned behavioral priors as extremely effective exploration priors for RL . VPT could even be a better general representation learning method even when the downstream task is not learning to act in that domain — for example , fine - tuning to explain what is happening in a video — because arguably the most important information in any given scene would be present in features trained to correctly predict the distribution over future human actions . We leave this intriguing direction to future work . Future work could improve results with more data ( we estimate we could collect > 1 M hours ) and larger , better - tuned models . Furthermore , all the models in this work condition on past observations only ; we can not ask the model to perform specific tasks . Appendix I presents preliminary experiments on conditioning our models on closed captions ( text transcripts of speech in videos ) , showing they 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rag_retrieve(query, embedding_model, vectore_store, top_k):\n",
    "    # embedd the query\n",
    "    embedded_query = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # dot product (cosine similarity because vectors are normalized)\n",
    "    scores = util.dot_score(a=embedded_query, b=vectore_store)[0]\n",
    "    # get the top k results\n",
    "    scores, indices = torch.topk(input=scores, k=top_k)\n",
    "    return scores, indices\n",
    "\n",
    "def show_retrieval_results(data_dict, query, scores, indices):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\\n\")\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print file path the page number\n",
    "        print(f\"File path: {data_dict[index]['file_path']}\")\n",
    "        print(f\"Page number: {data_dict[index]['page_number']}\")\n",
    "        # Print relevant sentence chunk\n",
    "        print(\"Text:\")\n",
    "        print(data_dict[index][\"sentence_chunk\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# test\n",
    "query = \"VPT video pre-training\"\n",
    "scores, indices = rag_retrieve(query, embedding_model, vectore_store=embeddings, top_k=2)\n",
    "show_retrieval_results(data_index, query, scores, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8608297-2250-464e-8875-c3f9ce5a0266",
   "metadata": {},
   "source": [
    "#### **Augmented Generation** \n",
    "Here we build a pipeline that starts with a prompt from the user, then it gets merged with another base prompt that is supported with **few-shot prompting** (in context learning) to better responses. Then a **similarity search** will happen between the embedded user query and the embeddings in the vector store to retrieve the **relevant chunks**. These chunks are added to the formatted prompt as a **context**. Then we prompt our **local LLM** to generate an answer. The generation is **streamed** and the relevant chunks are returned. Finally, we make sure to **empty the cache** of the GPU after each generation to not face memory issues (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16a68656-b5b6-460a-a26a-567f8718b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an explanation of attention:\n",
      "\n",
      "Attention is a mechanism in artificial intelligence (AI) that allows a neural network to focus on specific parts of the input data that are most relevant to a particular task. It involves calculating the weighted sum of the product of the query and each key, where the weights are determined by the attention score. The attention score is a measure of how much weight to assign to each key.\n",
      "\n",
      "In the context, attention is used in the encoder self-attention layer of a neural network. The encoder self-attention layer is a type of self-attention that is used to learn long-range dependencies in the input data.\n",
      "\n",
      "The output of the attention layer is a weighted sum of the dot products between the query and each key, where the weights are determined by the attention score. The attention scores are calculated by comparing the query to each key and then selecting the key that is most similar to the query.\n",
      "\n",
      "The attention mechanism allows the model to focus on the most relevant parts of the input data, which can lead to improved performance on tasks such as natural language processing (NLP) and machine translation.\n",
      "\n",
      " Resource 1: \n",
      "File path: ../data\\\\attention is all you need.pdf \n",
      "Page: 12 \n",
      "\n",
      "Resource 2: \n",
      "File path: ../data\\\\attention is all you need.pdf \n",
      "Page: 13 \n",
      "\n",
      "Resource 3: \n",
      "File path: ../data\\\\attention is all you need.pdf \n",
      "Page: 2 \n",
      "\n",
      "Resource 4: \n",
      "File path: ../data\\\\attention is all you need.pdf \n",
      "Page: 4 \n",
      "\n",
      "Resource 5: \n",
      "File path: ../data\\\\attention is all you need.pdf \n",
      "Page: 13 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_augmented_prompt(query, relevant_chunks, tokenizer):\n",
    "    \"\"\"\n",
    "    function to better format the prompt:\n",
    "    - use few-shot prompting (in context learning)\n",
    "    - add context from relevant chunks (augmentation)\n",
    "    \"\"\"\n",
    "\n",
    "    # join relevant chunks in one context string\n",
    "    chunks = [chunk[\"sentence_chunk\"] for chunk in relevant_chunks]\n",
    "    chunks = \" -\" + \"\\n -\".join(chunks)\n",
    "\n",
    "    # few-shot prompting\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "     Don't return the thinking, only return the answer.\n",
    "     Make sure your answers are as explanatory as possible.\n",
    "     Use the following example as a reference for the ideal answer style.\n",
    "     \\nExample 1:\n",
    "     Query: What is the role of backpropagation in neural networks?\n",
    "     Answer: Backpropagation is a key algorithm used for training neural networks by minimizing the error between predicted and actual outputs. It involves a forward pass where the input data is propagated through the network to generate an output, and a backward pass where the error is propagated back through the network to update the weights. This is done using the gradient descent optimization method, which calculates the gradient of the loss function with respect to each weight and adjusts the weights to reduce the error. Backpropagation allows neural networks to learn complex patterns in data by iteratively improving the model's accuracy.\n",
    "     \\nNow use the following context items to answer the user query:\n",
    "     {context}\n",
    "     \\nRelevant passages: <extract relevant passages from the context here>\n",
    "     \\nUser query: {query}\n",
    "     Answer:\"\"\"\n",
    "\n",
    "    # Add relevant chunks\n",
    "    base_prompt = base_prompt.format(context=chunks, query=query)\n",
    "    # final prompt, suited for instruction-tuned models\n",
    "    template = [{\"role\": \"user\", \"content\": base_prompt}]\n",
    "    # add_generation_prompt argument tells the template to add tokens that indicate the start of a bot response\n",
    "    prompt = tokenizer.apply_chat_template(conversation=template, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def augmented_generation(query, embedding_model, vector_store, data_index,\n",
    "                         top_k, similarity_threshold, llm_model, tokenizer, temperature, max_new_tokens, device):\n",
    "    # query your RAG to get relevant text\n",
    "    scores, indices = rag_retrieve(query=query, embedding_model=embedding_model, vectore_store=vector_store,\n",
    "                                   top_k=top_k)\n",
    "\n",
    "    # only keep chunks with scores higher than the similarity threshold\n",
    "    filtered_indices = [index for score, index in zip(scores, indices) if score > similarity_threshold]\n",
    "    relevant_chunks = [data_index[i] for i in filtered_indices]\n",
    "\n",
    "    # prepare the prompt\n",
    "    prompt = prepare_augmented_prompt(query=query, relevant_chunks=relevant_chunks, tokenizer=tokenizer)\n",
    "\n",
    "    # prompt the LLM\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # for streaming the response\n",
    "    response_streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    generation_kwargs = dict(**input_ids, streamer=response_streamer,\n",
    "                             temperature=temperature,\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # for streaming we run the generation in a different thread\n",
    "    thread = Thread(target=llm_model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # join retrieved resources in one text\n",
    "    retrieved_resources = \"\"\n",
    "    for i, source in enumerate(relevant_chunks):\n",
    "        retrieved_resources += (f\"Resource {i + 1}: \\n\"\n",
    "                                f\"File path: {source['file_path']} \\n\"\n",
    "                                f\"Page: {source['page_number']} \\n\\n\")\n",
    "    if not retrieved_resources:\n",
    "        retrieved_resources = \"No resources found for your query!\"\n",
    "    return response_streamer, retrieved_resources\n",
    "\n",
    "\n",
    "query = \"Explain attention in detail\"\n",
    "# Clear GPU cache before generation\n",
    "torch.cuda.empty_cache()\n",
    "streamer, retrieved_resources = augmented_generation(query=query, embedding_model=embedding_model,\n",
    "                                                     vector_store=embeddings, data_index=data_index,\n",
    "                                                     top_k=NUM_OF_RELEVANT_CHUNKS,\n",
    "                                                     similarity_threshold=SIMILARITY_THRESHOLD,\n",
    "                                                     llm_model=llm_model,\n",
    "                                                     tokenizer=tokenizer,\n",
    "                                                     temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS,\n",
    "                                                     device=DEVICE)\n",
    "\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\") \n",
    "print(\"\\n\\n\", retrieved_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adf0bd-5fcf-4186-8c03-38830a868f4d",
   "metadata": {},
   "source": [
    "#### **Run Gradio Web GUI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9132d835-b0b4-4879-87e1-2e957f153a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:51031\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:51031/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_answer(query):\n",
    "    # Clear GPU cache before generation\n",
    "    torch.cuda.empty_cache()\n",
    "    streamer, retrieved_resources = augmented_generation(query=query, embedding_model=embedding_model,\n",
    "                                                         vector_store=embeddings, data_index=data_index,\n",
    "                                                         top_k=NUM_OF_RELEVANT_CHUNKS,\n",
    "                                                         similarity_threshold=SIMILARITY_THRESHOLD,\n",
    "                                                         llm_model=llm_model,\n",
    "                                                         tokenizer=tokenizer,\n",
    "                                                         temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS,\n",
    "                                                         device=DEVICE)\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        yield generated_text, retrieved_resources\n",
    "\n",
    "# Launch the app\n",
    "theme = gr.themes.Default()\n",
    "demo = utils.gradio_rag_blocks(title=\"Chat With Your Data! (Local GPU)\",\n",
    "                                      description=\"Ask your documents using my local \" \\\n",
    "                                                  \"Retrieval-Augmented Generation (RAG) pipeline.\",\n",
    "                                      submit_fun=rag_answer,\n",
    "                                      theme=theme)\n",
    "free_port = utils.get_free_port()\n",
    "demo.launch(server_port=free_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76724d09-f3d1-4815-ba83-3fc54338161c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
