{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:21:45.015797Z",
     "start_time": "2024-08-12T15:21:41.200052Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "directory_path = (\n",
    "    \"data/\"\n",
    ")\n",
    "loader = PyPDFDirectoryLoader(directory_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:21:45.028740Z",
     "start_time": "2024-08-12T15:21:45.021634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:21:45.071919Z",
     "start_time": "2024-08-12T15:21:45.031660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press,\n",
      "2018.\n",
      "[12] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ ebiak, Christy\n",
      "Den\n",
      "{'source': 'data/vpt.pdf', 'page': 10}\n"
     ]
    }
   ],
   "source": [
    "print(docs[40].page_content[0:200])\n",
    "print(docs[40].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:21:53.380929Z",
     "start_time": "2024-08-12T15:21:45.039462Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "openai = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:23:30.212251Z",
     "start_time": "2024-08-12T15:23:25.267909Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:26:47.258307Z",
     "start_time": "2024-08-12T15:26:40.418829Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about how the VPT model is trained?',\n",
       " 'context': [Document(metadata={'page': 3, 'source': 'data/vpt.pdf'}, page_content='Collecting “Clean” Data Training the VPT Foundation Model\\nvia Behavioral Cloning\\nTraining the Inverse Dynamics Model (IDM)~270k hours\\nunlabeled\\nvideo~70k hours\\nunlabeled\\nvideo\\n~2k hours\\nvideo\\nlabeled with\\nactionsFilter for “clean”\\nvideo segmentsSearch for relevant\\nMinecraft videos\\nvia keywords\\nContractors\\ncollect data Label videos\\nwith IDM ~70k hours\\nvideo\\nIDM-labeled\\nwith actions\\nTrain non-causal IDM\\nTrain causal\\nVPT Foundation Model\\nadspace\\nwadspace\\nwFigure 2: Video Pretraining (VPT) Method Overview.\\n3 Methods\\nInverse Dynamics Models (IDM) VPT, illustrated in Figure 2, requires we ﬁrst collect a small\\namount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1...T),\\nwhich seeks to minimize the negative log-likelihood of an action at timestep tgiven a trajectory of T\\nobservations ot:t∈[1...T]. In contrast to an imitation learning policy, the IDM can be non-causal,'),\n",
       "  Document(metadata={'page': 5, 'source': 'data/vpt.pdf'}, page_content='collects various berries and mushrooms and eats them; and ﬁnds game-generated villages from which\\nto collect various rare items from chests. The model also learned to navigate uneven terrain, swim,\\nand pillar jump, which involves the agent repeatedly jumping and quickly placing a block below itself\\nsuch that it climbs upward by making a pillar.(iv)\\nWhile training and validation loss decrease healthily over training (Fig. 4, left), loss on our contractor\\ndataset (which the VPT model does not train on) begins increasing after 7 epochs. Contractor data\\ncould be out-of-distribution because our contractors may have a different distribution of play or\\nbecause there is some impactful visual domain shift compared to videos from the web. While one\\ncould have expected this would be predictive of declining evaluation performance, we do not see\\nnotable game statistics from the VPT foundation model rollouts (Figure 4, right) decrease over'),\n",
       "  Document(metadata={'page': 9, 'source': 'data/vpt.pdf'}, page_content='become weakly steerable; we believe this a rich direction for future research. Also, loss was not\\nconsistently correlated with downstream evaluation metrics (Sec. 4.2), which often made progress\\nslow and hard-won. Another fruitful future direction would be to investigate the correlation between\\nvarious training metrics and downstream evaluations. Finally, while we do not anticipate any direct\\nnegative societal impacts from the models trained in this work, as VPT improves and expands to other\\ndomains it will be important to assess and mitigate harms that emerge with other forms of pretraining\\non internet datasets, such as emulating inappropriate behavior.67\\nIn conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from\\nfreely available internet-scale data to sequential decision domains. Our models exhibited impressive\\nzero-shot behavior and, when ﬁne-tuned with RL, achieved an unprecedented result of crafting a'),\n",
       "  Document(metadata={'page': 4, 'source': 'data/vpt.pdf'}, page_content='(Appendix A has further details on data scraping and ﬁltering). We then generated pseudo-labels\\nforweb_clean with our best IDM (Section 3) and then trained the VPT foundation model with\\nbehavioral cloning. Preliminary experiments suggested that our model could beneﬁt from 30 epochs\\nof training and that a 0.5 billion parameter model was required to stay in the efﬁcient learning\\nregime63for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs.\\nWe evaluate our models by measuring validation loss (Fig. 4, left) and rolling them out in the\\nMinecraft environment. Unless otherwise noted, in all environment evaluations we spawn agents in a\\nstandard survival mode game where they play for 60 minutes, i.e. 72000 consecutive actions, and we\\nplot the mean and shade the standard error of the mean for various game statistics such as crafting\\nand collection rates (Fig. 4, right). The VPT foundation model quickly learns to chop down trees')],\n",
       " 'answer': \"The VPT model is trained using a small amount of labeled contractor data to develop an inverse dynamics model (IDM), which minimizes the negative log-likelihood of actions based on observed trajectories. After collecting clean video segments and labeling them with actions, the model undergoes training via behavioral cloning. The training process involves evaluating the model's performance through validation loss and rolling it out in environments like Minecraft for practical assessments.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(openai, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"Tell me about how the VPT model is trained?\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T16:13:48.177318Z",
     "start_time": "2024-08-12T16:13:39.408878Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "from pydantic import PydanticDeprecatedSince20\n",
    "# Ignore specific warning from logger\n",
    "warnings.filterwarnings(\"ignore\", category=PydanticDeprecatedSince20)\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# get openAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# load the documents\n",
    "directory_path = \"data/\"\n",
    "loader = PyPDFDirectoryLoader(directory_path)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Docuemnt Chunking, Create Embedding,  Build vector-store\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "# load OpenAI LLM API\n",
    "llm_gpt4o_mini = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# create the prompt template\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# build up the RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is steve and its relationship to the VPT model?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T16:13:51.706387Z",
     "start_time": "2024-08-12T16:13:51.702028Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
