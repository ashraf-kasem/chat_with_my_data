{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f92cd5-970b-4763-b0b7-e34ccb9be97b",
   "metadata": {},
   "source": [
    "## **Build the RAG**\n",
    "We have already created a vector store that holds the locally created embeddings, now we need to have Q & A pipeline with augmented-prompts by our data files and LLM that could be hosted locally (on my GPU) or in the cloud using an API (openAI).  \n",
    "\n",
    "#### **Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f5cba5-05a7-4f5b-be2d-b91520f2a31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>1165</td>\n",
       "      <td>154</td>\n",
       "      <td>291.25</td>\n",
       "      <td>[ 2.07077805e-02  2.70413030e-02 -1.68691296e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Our model achieves 28.4 BLEU on the WMT 2014 E...</td>\n",
       "      <td>620</td>\n",
       "      <td>95</td>\n",
       "      <td>155.00</td>\n",
       "      <td>[ 1.10328430e-03  5.08999750e-02  3.29319574e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Jakob proposed replacing RNNs with self-attent...</td>\n",
       "      <td>658</td>\n",
       "      <td>90</td>\n",
       "      <td>164.50</td>\n",
       "      <td>[ 1.54208224e-02  1.14086864e-03 -6.22348813e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Lukasz and Aidan spent countless long days des...</td>\n",
       "      <td>392</td>\n",
       "      <td>49</td>\n",
       "      <td>98.00</td>\n",
       "      <td>[ 2.14229785e-02  5.34767583e-02 -1.31562511e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction Recurrent neural networks, long...</td>\n",
       "      <td>1115</td>\n",
       "      <td>158</td>\n",
       "      <td>278.75</td>\n",
       "      <td>[-2.53893668e-03  4.21451516e-02 -5.52566350e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file_path  page_number  \\\n",
       "0  data\\\\attention is all you need.pdf            0   \n",
       "1  data\\\\attention is all you need.pdf            0   \n",
       "2  data\\\\attention is all you need.pdf            0   \n",
       "3  data\\\\attention is all you need.pdf            0   \n",
       "4  data\\\\attention is all you need.pdf            1   \n",
       "\n",
       "                                      sentence_chunk  chunk_char_count  \\\n",
       "0  Provided proper attribution is provided, Googl...              1165   \n",
       "1  Our model achieves 28.4 BLEU on the WMT 2014 E...               620   \n",
       "2  Jakob proposed replacing RNNs with self-attent...               658   \n",
       "3  Lukasz and Aidan spent countless long days des...               392   \n",
       "4  1 Introduction Recurrent neural networks, long...              1115   \n",
       "\n",
       "   chunk_word_count  chunk_token_count  \\\n",
       "0               154             291.25   \n",
       "1                95             155.00   \n",
       "2                90             164.50   \n",
       "3                49              98.00   \n",
       "4               158             278.75   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 2.07077805e-02  2.70413030e-02 -1.68691296e-...  \n",
       "1  [ 1.10328430e-03  5.08999750e-02  3.29319574e-...  \n",
       "2  [ 1.54208224e-02  1.14086864e-03 -6.22348813e-...  \n",
       "3  [ 2.14229785e-02  5.34767583e-02 -1.31562511e-...  \n",
       "4  [-2.53893668e-03  4.21451516e-02 -5.52566350e-...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the embeddings from our local vector store\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embeddings_df_save_path = \"vector_store/embeddings.csv\"\n",
    "embeddings_loaded_df = pd.read_csv(embeddings_df_save_path)\n",
    "embeddings_loaded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0d06d-6521-4d51-857d-ff56e9cd539d",
   "metadata": {},
   "source": [
    "Do some conversions on the embeddings dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774834cf-3cfb-4206-af7b-c0360e8a7b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([450, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Convert embedding column back to np.array if they were string\n",
    "if  isinstance ( embeddings_loaded_df[\"embedding\"][0], str):\n",
    "    embeddings_loaded_df[\"embedding\"] = embeddings_loaded_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts (useful later)\n",
    "embeddings_loaded_dict = embeddings_loaded_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device\n",
    "embeddings = torch.tensor(np.array(embeddings_loaded_df[\"embedding\"].tolist()), dtype=torch.float32).to(DEVICE)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73c139d-09d1-498d-b701-d408bc0f5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed9f78d-fdd9-4355-a7a4-1412a875b0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.5689, 0.4328, 0.3861], device='cuda:0'),\n",
       "indices=tensor([  9,  86, 100], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do similarity search between a query and the the vector store, return best 3 matches\n",
    "\n",
    "query = \"what are the main layers of the transformer? \"\n",
    "# embedd the query\n",
    "embedded_query = embedding_model.encode(query, convert_to_tensor=True)\n",
    "# get a similarity score\n",
    "dot_scores = util.dot_score(a=embedded_query, b=embeddings)[0]\n",
    "\n",
    "# get the top 3 matches\n",
    "top_results_dot_product = torch.topk(dot_scores, k=3)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945fa2b6-a0b3-4844-b9ea-8c7112e76e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8b1333-4f0f-4fff-97eb-3f2ccee5c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'what are the main layers of the transformer? '\n",
      "\n",
      "Results: \n",
      "\n",
      "Score: 0.5689\n",
      "Page number: 2\n",
      "File path: data\\\\attention is all you need.pdf\n",
      "Text:\n",
      "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network.\n",
      "\n",
      "\n",
      "Score: 0.4328\n",
      "Page number: 3\n",
      "File path: data\\\\CNN.pdf\n",
      "Text:\n",
      "2.1 Overall architecture CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers. When these layers are stacked, a CNN architecture has been formed. A simpliﬁed CNN architecture for MNIST classiﬁcation is illustrated in Figure 2.input 0 9 convolution w/ReLu pooling output fully-connected w/ ReLu fully-connected ... Fig.\n",
      "\n",
      "\n",
      "Score: 0.3861\n",
      "Page number: 7\n",
      "File path: data\\\\CNN.pdf\n",
      "Text:\n",
      "General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primar- ily focus on the use of max-pooling.2.4 Fully-connected layer The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1) 3 Recipes Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  use results to map back to our text chunks\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results: \\n\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {embeddings_loaded_dict[idx]['page_number']}\")\n",
    "    print(f\"File path: {embeddings_loaded_dict[idx]['file_path']}\")\n",
    "    # Print relevant sentence chunk \n",
    "    print(\"Text:\")\n",
    "    print(embeddings_loaded_dict[idx][\"sentence_chunk\"])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e655e82-ae8b-4a58-9b5c-1b3544ec27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retrieve(query, embedding_model, vectore_store, top_k):\n",
    "    # embedd the query\n",
    "    embedded_query = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # dot product (cosine similarity because vectors are normalized) \n",
    "    scores = util.dot_score(a=embedded_query, b=vectore_store)[0]\n",
    "    # get the top k results\n",
    "    scores, indices = torch.topk(input=scores, k=top_k)\n",
    "    return scores, indices\n",
    "\n",
    "def show_retrieval_results(data_dict, query, scores, indices):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\\n\")\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print file path the page number \n",
    "        print(f\"File path: {data_dict[index]['file_path']}\")\n",
    "        print(f\"Page number: {data_dict[index]['page_number']}\")\n",
    "        # Print relevant sentence chunk \n",
    "        print(\"Text:\")\n",
    "        print(data_dict[index][\"sentence_chunk\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad2a8bf-8e3f-4c5b-860e-4d9cd4305e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5530, 0.5499, 0.5483, 0.5409], device='cuda:0') \n",
      " tensor([279, 417, 305, 271], device='cuda:0') \n",
      "\n",
      "Query: vpt: video pre-training, hyperparameters \n",
      "\n",
      "Results:\n",
      "\n",
      "Score: 0.5530\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 4\n",
      "Text:\n",
      "These searches resulted in ∼270k hours of video, which we filtered down to “clean” video segments yielding an unlabeled dataset of ∼70k hours, which we refer to as web_clean (Appendix A has further details on data scraping and filtering). We then generated pseudo-labels for web_clean with our best IDM (Section 3) and then trained the VPT foundation model with behavioral cloning. Preliminary experiments suggested that our model could benefit from 30 epochs of training and that a 0.5 billion parameter model was required to stay in the efficient learning regime63 for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs. We evaluate our models by measuring validation loss (Fig.4, left) and rolling them out in the Minecraft environment.\n",
      "\n",
      "\n",
      "Score: 0.5499\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 25\n",
      "Text:\n",
      "The hyperparameters used for foundation model training are listed in Table 4. Hyperparameter Value Learning rate 0.002147 Weight decay 0.0625 Epochs 30 Batch size 880 Table 4: Hyperparameters for foundation model training F Behavioral Cloning Fine-Tuning Behavior cloning fine-tuning is similar to the foundation model training, except we either use a focused subset of all the videos (early_game dataset, described in A.3) with pseudo labels, or contractor data (contractor_house dataset, described in B.4) with ground-truth labels. The hyperparameters used for behavior cloning fine-tuning are listed in Table 5. We used 16 A100 GPUs for about 6 hours when fine-tuning on contractor_house dataset, and 16 A100 GPUs for about 2 days when fine-tuning on early_game dataset. Hyperparameter Value Learning rate 0.000181 Weight decay 0.039428 Epochs 2 Batch size 16 Table 5: Hyperparameters for behavior cloning fine-tuning G Reinforcement Learning Fine-Tuning G.1 Reinforcement Learning Fine-Tuning Training Details RL experiments were performed with the phasic policy gradient (PPG) algorithm,64 an RL algorithm based on the proximal policy optimization (PPO) algorithm77 that increases sample efficiency by performing additional passes over the collected data to optimize the value function as well as an 26\n",
      "\n",
      "\n",
      "Score: 0.5483\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 9\n",
      "Text:\n",
      "become weakly steerable; we believe this a rich direction for future research. Also, loss was not consistently correlated with downstream evaluation metrics (Sec.4.2), which often made progress slow and hard-won. Another fruitful future direction would be to investigate the correlation between various training metrics and downstream evaluations. Finally, while we do not anticipate any direct negative societal impacts from the models trained in this work, as VPT improves and expands to other domains it will be important to assess and mitigate harms that emerge with other forms of pretraining on internet datasets, such as emulating inappropriate behavior.67 In conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from freely available internet-scale data to sequential decision domains.\n",
      "\n",
      "\n",
      "Score: 0.5409\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 3\n",
      "Text:\n",
      "Collecting “Clean” Data Training the VPT Foundation Model via Behavioral Cloning Training the Inverse Dynamics Model (IDM) ~270k hours unlabeled video ~70k hours unlabeled video ~2k hours video labeled with actions Filter for “clean” video segments Search for relevant Minecraft videos via keywords Contractors collect data Label videos with IDM ~70k hours video IDM-labeled with actions Train non-causal IDM Train causal VPT Foundation Model a d space w a d space w Figure 2: Video Pretraining (VPT) Method Overview.3 Methods Inverse Dynamics Models (IDM) VPT, illustrated in Figure 2, requires we first collect a small amount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1... T ), which seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T observations ot : t ∈ [1... T]. In contrast to an imitation learning policy, the IDM can be non-causal, meaning its prediction for at can be a function of both past and future events, i.e. ot′>t. Compared to the behavioral cloning objective of modeling the distribution of human intent given past frames only, we hypothesize that inverting environment dynamics is easier and more data efficient to learn. Indeed, Sec.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"vpt: video pre-training, hyperparameters \"\n",
    "scores, indices = rag_retrieve(query, embedding_model, embeddings, 4)\n",
    "print(scores, \"\\n\",  indices, \"\\n\")\n",
    "show_retrieval_results(embeddings_loaded_dict, query, scores, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e6a61-c45a-46a1-9aef-4fe07cd6143a",
   "metadata": {},
   "source": [
    "#### **Loading an LLM locally** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71df488-2501-44b5-a5f1-55d511f432cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 6 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9957df87-4e64-4895-90c0-9cfa67a9b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My GPU is Nvidia RTX 3060 with 6GB memory\n",
    "# Loading 2 Billion  parameters model in full precision needs 2b * 4 ~ 8GB of GPU memory\n",
    "# I need to do quantization to float-16 or int-8\n",
    "model_id  = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd92f89-97c3-4a41-958b-d011e6c0df1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b7137ddf314ab298c458d0d6a05173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downlaod and load the model for inference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# load in 4bit precision (boost the inference time significantly) \n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 low_cpu_mem_usage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc15026c-3460-406c-a30b-b1ca10cf99a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcadaf2-dc29-4761-aa5a-fd9e54ef909b",
   "metadata": {},
   "source": [
    "from this we can notice:\n",
    "- vocab size is 256k\n",
    "- hidden size is 2048\n",
    "- context length from the model card 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4806f1e-27d5-4eaf-9b2b-7e9d9aed9ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9a8703-ef23-4856-a846-0836da0fbdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "what is attention as described in the attention all you need paper? \n",
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   5049,    603,   6137,    685,\n",
      "           6547,    575,    573,   6137,    832,    692,   1476,   4368, 235336,\n",
      "            107,    108,    106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   5049,    603,   6137,    685,\n",
      "          6547,    575,    573,   6137,    832,    692,   1476,   4368, 235336,\n",
      "           107,    108,    106,   2516,    108,    886,    573,  19422,    576,\n",
      "           573,  50289,   2262,   1646,  15213,   4368, 235269,   6137,    603,\n",
      "          6908,    685,   6397, 235292,    109,    688,  75927,   6137,  66058,\n",
      "          1417,   8563,    573,   2091,    577,  27104,   2369,   2113,    774,\n",
      "          2167,   4942,    576,    573,   3772,  10629, 235269,   4998,   1280,\n",
      "          3185,    573,   8761,   3668,    576,   3907,    575,    573,  10629,\n",
      "        235265,   1417,   7154,    577,  16446,   1497, 235290,   4201,  43634,\n",
      "          1865,   3907, 235269,    948,    708,   3695,  20305,    604,  13333,\n",
      "          1582,    685,   5255,  17183, 235269,   6479,  17183, 235269,    578,\n",
      "          2872,  39534, 235265,    109,    688, 129780,  12846, 235290,   6088,\n",
      "          6137,  66058,   1417,    603,    476,   3724,   1916,    576,   6137,\n",
      "           674,  95035,    573,  12846, 235290,   6088,   6791,    577,   4015,\n",
      "           573,  40174,   1865,   1378,  27682, 235265,   1417,   6791,   8563,\n",
      "           573,   2091,    577,   3508,   1368,   2167,   4942,    576,    573,\n",
      "          3772,  10629,   2613,    614,   5678,    577,   1853,   1156, 235265,\n",
      "           109,    688, 129780,   5228,   6137,  66058,   1417,   6137,  15613,\n",
      "         42451,    573,  30854,    576,  51870,  12846, 235290,   6088,   6137,\n",
      "          1163,    832,    573,  18549,    576,   3907,    575,    573,  10629,\n",
      "        235265,   1417,  11845,    577,    476,    978,   5228,   8377,    576,\n",
      "           573,   3772,    578,   8563,    573,   2091,    577,  16446,  25210,\n",
      "         12733,   1865,   3907, 235265,    109,    688, 117414,   6137,  66058,\n",
      "          1417,  21340,    576,   6137,  19950,    476, 106083,    577,   1853,\n",
      "          8537,    576,   3772,   5119,   3482,    611,   1024,  40174, 235265,\n",
      "          1417,   8563,    573,   2091,    577,   2734,    978,   9156,    577,\n",
      "          5119,    674,    708,    978,   9666,    577,    573,   6911,    696,\n",
      "          1634, 235265,    109,    688,   9886, 235290,  10904,   6137,  66058,\n",
      "          1417,   1916,    576,   6137,  27329,   2113,    774,   2167, 122068,\n",
      "           591, 235249, 235265, 235264,   1173,   2793,    578,   5191, 235275,\n",
      "           731,  51506,  25737,    577,   9666,   4942,    576,    573,   3772],\n",
      "       device='cuda:0')\n",
      "\n",
      "Model output (decoded):\n",
      "user\n",
      "what is attention as described in the attention all you need paper?\n",
      "model\n",
      "In the Context of the Attention All You Need paper, attention is defined as follows:\n",
      "\n",
      "**Spatial attention:** This allows the model to simultaneously consider information from different parts of the input sequence, taking into account the relative position of words in the sequence. This helps to capture long-range dependencies between words, which are often crucial for tasks such as language translation, machine translation, and question answering.\n",
      "\n",
      "**Scaled dot-product attention:** This is a specific type of attention that utilizes the dot-product operation to measure the similarity between two vectors. This operation allows the model to understand how different parts of the input sequence might be related to each other.\n",
      "\n",
      "**Scaled global attention:** This attention mechanism combines the outputs of scaled dot-product attention over all the pairs of words in the sequence. This leads to a more global understanding of the input and allows the model to capture distant relationships between words.\n",
      "\n",
      "**Weighted attention:** This variant of attention applies a weighting to each pair of input features based on their similarity. This allows the model to give more importance to features that are more relevant to the task at hand.\n",
      "\n",
      "**Multi-modal attention:** This type of attention considers information from different modalities (e.g., text and images) by jointly attending to relevant parts of the input\n",
      "\n",
      "CPU times: total: 10.9 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_text = \"what is attention as described in the attention all you need paper? \"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256,\n",
    "                            do_sample=True) \n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")\n",
    "\n",
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode( outputs[0], skip_special_tokens=True)\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d2e60-22d1-4473-b866-d3d77e7a3f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657096c-a17e-456d-8b85-5a5914931a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
