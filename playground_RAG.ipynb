{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f92cd5-970b-4763-b0b7-e34ccb9be97b",
   "metadata": {},
   "source": [
    "## **Build the RAG**\n",
    "We have already created a vector store that holds the locally created embeddings, now we need to have Q & A pipeline with augmented-prompts by our data files and LLM that could be hosted locally (on my GPU) or in the cloud using an API (openAI).  \n",
    "\n",
    "#### **Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f5cba5-05a7-4f5b-be2d-b91520f2a31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>1165</td>\n",
       "      <td>154</td>\n",
       "      <td>291.25</td>\n",
       "      <td>[ 2.07077805e-02  2.70413030e-02 -1.68691296e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Our model achieves 28.4 BLEU on the WMT 2014 E...</td>\n",
       "      <td>620</td>\n",
       "      <td>95</td>\n",
       "      <td>155.00</td>\n",
       "      <td>[ 1.10328430e-03  5.08999750e-02  3.29319574e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Jakob proposed replacing RNNs with self-attent...</td>\n",
       "      <td>658</td>\n",
       "      <td>90</td>\n",
       "      <td>164.50</td>\n",
       "      <td>[ 1.54208224e-02  1.14086864e-03 -6.22348813e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Lukasz and Aidan spent countless long days des...</td>\n",
       "      <td>392</td>\n",
       "      <td>49</td>\n",
       "      <td>98.00</td>\n",
       "      <td>[ 2.14229785e-02  5.34767583e-02 -1.31562511e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\\\attention is all you need.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction Recurrent neural networks, long...</td>\n",
       "      <td>1115</td>\n",
       "      <td>158</td>\n",
       "      <td>278.75</td>\n",
       "      <td>[-2.53893668e-03  4.21451516e-02 -5.52566350e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file_path  page_number  \\\n",
       "0  data\\\\attention is all you need.pdf            0   \n",
       "1  data\\\\attention is all you need.pdf            0   \n",
       "2  data\\\\attention is all you need.pdf            0   \n",
       "3  data\\\\attention is all you need.pdf            0   \n",
       "4  data\\\\attention is all you need.pdf            1   \n",
       "\n",
       "                                      sentence_chunk  chunk_char_count  \\\n",
       "0  Provided proper attribution is provided, Googl...              1165   \n",
       "1  Our model achieves 28.4 BLEU on the WMT 2014 E...               620   \n",
       "2  Jakob proposed replacing RNNs with self-attent...               658   \n",
       "3  Lukasz and Aidan spent countless long days des...               392   \n",
       "4  1 Introduction Recurrent neural networks, long...              1115   \n",
       "\n",
       "   chunk_word_count  chunk_token_count  \\\n",
       "0               154             291.25   \n",
       "1                95             155.00   \n",
       "2                90             164.50   \n",
       "3                49              98.00   \n",
       "4               158             278.75   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 2.07077805e-02  2.70413030e-02 -1.68691296e-...  \n",
       "1  [ 1.10328430e-03  5.08999750e-02  3.29319574e-...  \n",
       "2  [ 1.54208224e-02  1.14086864e-03 -6.22348813e-...  \n",
       "3  [ 2.14229785e-02  5.34767583e-02 -1.31562511e-...  \n",
       "4  [-2.53893668e-03  4.21451516e-02 -5.52566350e-...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the embeddings from our local vector store\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embeddings_df_save_path = \"vector_store/embeddings.csv\"\n",
    "embeddings_loaded_df = pd.read_csv(embeddings_df_save_path)\n",
    "embeddings_loaded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0d06d-6521-4d51-857d-ff56e9cd539d",
   "metadata": {},
   "source": [
    "Do some conversions on the embeddings dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774834cf-3cfb-4206-af7b-c0360e8a7b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([450, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Convert embedding column back to np.array if they were string\n",
    "if  isinstance ( embeddings_loaded_df[\"embedding\"][0], str):\n",
    "    embeddings_loaded_df[\"embedding\"] = embeddings_loaded_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts (useful later)\n",
    "embeddings_loaded_dict = embeddings_loaded_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device\n",
    "embeddings = torch.tensor(np.array(embeddings_loaded_df[\"embedding\"].tolist()), dtype=torch.float32).to(DEVICE)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73c139d-09d1-498d-b701-d408bc0f5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed9f78d-fdd9-4355-a7a4-1412a875b0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.5689, 0.4328, 0.3861], device='cuda:0'),\n",
       "indices=tensor([  9,  86, 100], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do similarity search between a query and the the vector store, return best 3 matches\n",
    "\n",
    "query = \"what are the main layers of the transformer? \"\n",
    "# embedd the query\n",
    "embedded_query = embedding_model.encode(query, convert_to_tensor=True)\n",
    "# get a similarity score\n",
    "dot_scores = util.dot_score(a=embedded_query, b=embeddings)[0]\n",
    "\n",
    "# get the top 3 matches\n",
    "top_results_dot_product = torch.topk(dot_scores, k=3)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945fa2b6-a0b3-4844-b9ea-8c7112e76e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8b1333-4f0f-4fff-97eb-3f2ccee5c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'what are the main layers of the transformer? '\n",
      "\n",
      "Results: \n",
      "\n",
      "Score: 0.5689\n",
      "Page number: 2\n",
      "File path: data\\\\attention is all you need.pdf\n",
      "Text:\n",
      "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network.\n",
      "\n",
      "\n",
      "Score: 0.4328\n",
      "Page number: 3\n",
      "File path: data\\\\CNN.pdf\n",
      "Text:\n",
      "2.1 Overall architecture CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers. When these layers are stacked, a CNN architecture has been formed. A simpliﬁed CNN architecture for MNIST classiﬁcation is illustrated in Figure 2.input 0 9 convolution w/ReLu pooling output fully-connected w/ ReLu fully-connected ... Fig.\n",
      "\n",
      "\n",
      "Score: 0.3861\n",
      "Page number: 7\n",
      "File path: data\\\\CNN.pdf\n",
      "Text:\n",
      "General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primar- ily focus on the use of max-pooling.2.4 Fully-connected layer The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1) 3 Recipes Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  use results to map back to our text chunks\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results: \\n\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {embeddings_loaded_dict[idx]['page_number']}\")\n",
    "    print(f\"File path: {embeddings_loaded_dict[idx]['file_path']}\")\n",
    "    # Print relevant sentence chunk \n",
    "    print(\"Text:\")\n",
    "    print(embeddings_loaded_dict[idx][\"sentence_chunk\"])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e655e82-ae8b-4a58-9b5c-1b3544ec27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retrieve(query, embedding_model, vectore_store, top_k):\n",
    "    # embedd the query\n",
    "    embedded_query = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # dot product (cosine similarity because vectors are normalized) \n",
    "    scores = util.dot_score(a=embedded_query, b=vectore_store)[0]\n",
    "    # get the top k results\n",
    "    scores, indices = torch.topk(input=scores, k=top_k)\n",
    "    return scores, indices\n",
    "\n",
    "def show_retrieval_results(data_dict, query, scores, indices):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\\n\")\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print file path the page number \n",
    "        print(f\"File path: {data_dict[index]['file_path']}\")\n",
    "        print(f\"Page number: {data_dict[index]['page_number']}\")\n",
    "        # Print relevant sentence chunk \n",
    "        print(\"Text:\")\n",
    "        print(data_dict[index][\"sentence_chunk\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad2a8bf-8e3f-4c5b-860e-4d9cd4305e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5530, 0.5499, 0.5483, 0.5409], device='cuda:0') \n",
      " tensor([279, 417, 305, 271], device='cuda:0') \n",
      "\n",
      "Query: vpt: video pre-training, hyperparameters \n",
      "\n",
      "Results:\n",
      "\n",
      "Score: 0.5530\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 4\n",
      "Text:\n",
      "These searches resulted in ∼270k hours of video, which we filtered down to “clean” video segments yielding an unlabeled dataset of ∼70k hours, which we refer to as web_clean (Appendix A has further details on data scraping and filtering). We then generated pseudo-labels for web_clean with our best IDM (Section 3) and then trained the VPT foundation model with behavioral cloning. Preliminary experiments suggested that our model could benefit from 30 epochs of training and that a 0.5 billion parameter model was required to stay in the efficient learning regime63 for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs. We evaluate our models by measuring validation loss (Fig.4, left) and rolling them out in the Minecraft environment.\n",
      "\n",
      "\n",
      "Score: 0.5499\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 25\n",
      "Text:\n",
      "The hyperparameters used for foundation model training are listed in Table 4. Hyperparameter Value Learning rate 0.002147 Weight decay 0.0625 Epochs 30 Batch size 880 Table 4: Hyperparameters for foundation model training F Behavioral Cloning Fine-Tuning Behavior cloning fine-tuning is similar to the foundation model training, except we either use a focused subset of all the videos (early_game dataset, described in A.3) with pseudo labels, or contractor data (contractor_house dataset, described in B.4) with ground-truth labels. The hyperparameters used for behavior cloning fine-tuning are listed in Table 5. We used 16 A100 GPUs for about 6 hours when fine-tuning on contractor_house dataset, and 16 A100 GPUs for about 2 days when fine-tuning on early_game dataset. Hyperparameter Value Learning rate 0.000181 Weight decay 0.039428 Epochs 2 Batch size 16 Table 5: Hyperparameters for behavior cloning fine-tuning G Reinforcement Learning Fine-Tuning G.1 Reinforcement Learning Fine-Tuning Training Details RL experiments were performed with the phasic policy gradient (PPG) algorithm,64 an RL algorithm based on the proximal policy optimization (PPO) algorithm77 that increases sample efficiency by performing additional passes over the collected data to optimize the value function as well as an 26\n",
      "\n",
      "\n",
      "Score: 0.5483\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 9\n",
      "Text:\n",
      "become weakly steerable; we believe this a rich direction for future research. Also, loss was not consistently correlated with downstream evaluation metrics (Sec.4.2), which often made progress slow and hard-won. Another fruitful future direction would be to investigate the correlation between various training metrics and downstream evaluations. Finally, while we do not anticipate any direct negative societal impacts from the models trained in this work, as VPT improves and expands to other domains it will be important to assess and mitigate harms that emerge with other forms of pretraining on internet datasets, such as emulating inappropriate behavior.67 In conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from freely available internet-scale data to sequential decision domains.\n",
      "\n",
      "\n",
      "Score: 0.5409\n",
      "File path: data\\\\video pretraining VPT.pdf\n",
      "Page number: 3\n",
      "Text:\n",
      "Collecting “Clean” Data Training the VPT Foundation Model via Behavioral Cloning Training the Inverse Dynamics Model (IDM) ~270k hours unlabeled video ~70k hours unlabeled video ~2k hours video labeled with actions Filter for “clean” video segments Search for relevant Minecraft videos via keywords Contractors collect data Label videos with IDM ~70k hours video IDM-labeled with actions Train non-causal IDM Train causal VPT Foundation Model a d space w a d space w Figure 2: Video Pretraining (VPT) Method Overview.3 Methods Inverse Dynamics Models (IDM) VPT, illustrated in Figure 2, requires we first collect a small amount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1... T ), which seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T observations ot : t ∈ [1... T]. In contrast to an imitation learning policy, the IDM can be non-causal, meaning its prediction for at can be a function of both past and future events, i.e. ot′>t. Compared to the behavioral cloning objective of modeling the distribution of human intent given past frames only, we hypothesize that inverting environment dynamics is easier and more data efficient to learn. Indeed, Sec.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"vpt: video pre-training, hyperparameters \"\n",
    "scores, indices = rag_retrieve(query, embedding_model, embeddings, 4)\n",
    "print(scores, \"\\n\",  indices, \"\\n\")\n",
    "show_retrieval_results(embeddings_loaded_dict, query, scores, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e6a61-c45a-46a1-9aef-4fe07cd6143a",
   "metadata": {},
   "source": [
    "#### **Loading an LLM locally** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71df488-2501-44b5-a5f1-55d511f432cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 6 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9957df87-4e64-4895-90c0-9cfa67a9b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My GPU is Nvidia RTX 3060 with 6GB memory\n",
    "# Loading 2 Billion  parameters model in full precision needs 2b * 4 ~ 8GB of GPU memory\n",
    "# I need to do quantization to float-16 or int-8\n",
    "model_id  = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd92f89-97c3-4a41-958b-d011e6c0df1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9a1e45a197480ca2a2cab712e24f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downlaod and load the model for inference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# load in 4bit precision (boost the inference time significantly) \n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 low_cpu_mem_usage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc15026c-3460-406c-a30b-b1ca10cf99a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcadaf2-dc29-4761-aa5a-fd9e54ef909b",
   "metadata": {},
   "source": [
    "from this we can notice:\n",
    "- vocab size is 256k\n",
    "- hidden size is 2048\n",
    "- context length from the model card 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4806f1e-27d5-4eaf-9b2b-7e9d9aed9ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9a8703-ef23-4856-a846-0836da0fbdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "what is attention as described in the attention all you need paper? \n",
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   5049,    603,   6137,    685,\n",
      "           6547,    575,    573,   6137,    832,    692,   1476,   4368, 235336,\n",
      "            107,    108,    106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   5049,    603,   6137,    685,\n",
      "          6547,    575,    573,   6137,    832,    692,   1476,   4368, 235336,\n",
      "           107,    108,    106,   2516,    108,    886,    573,   4368,    664,\n",
      "         41261,    685,  20555,  22021, 235281,    731,  24556,  88085,   1008,\n",
      "           717, 235265,    591, 235284, 235276, 235274, 235324,    823,   6137,\n",
      "           603,   6547,    685,    476,  15613,    604,  25737,    577,   3383,\n",
      "          4942,    576,    573,   3772,  10629,    575,   2184,    577,   1501,\n",
      "          2525,  32794, 235265,   1417,  15613,   8563,    573,   2091,    577,\n",
      "         36570,   1277,   6044,    611,   4942,    576,    573,   3772,    674,\n",
      "           708,   9666,    577,    573,   6911,    696,   1634, 235265, 235248,\n",
      "           109,   4858, 235303, 235256,    476,  25497,    576,    573,   2621,\n",
      "          3782, 235292,    109, 235287,   5231,  41261,    603,    476,  15613,\n",
      "         95573,   1165,    603,    780,    476,   3821,   2091, 235269,    901,\n",
      "          4644,    476,   5488,    576,   2167,  22154,   2819,    573,  35544,\n",
      "          6011, 235265,    108, 235287,   5231,  41261,   8563,    573,   2091,\n",
      "           577,   6045,    611,    476,  38397,    576,    573,   3772,   1423,\n",
      "           688,    674,    603,   1546,   9666,    604,    573,   6911,    696,\n",
      "          1634, 235265,   1417,   8563,    573,   2091,    577,   3114,  13333,\n",
      "           674,   1134,   8481,    614,  11623,    604,    476,   2091,    674,\n",
      "          1297,  27329,    573,   4805,   3772,    696,   3631, 235265,    108,\n",
      "        235287,   5231,  41261,  31987,   2582,   1570,    573,   2091,  31381,\n",
      "          1277,   6137,  95573,   3766,  31987,    798,    614,  17363,    577,\n",
      "          3918,   3724,  22718,    604,   2167,   4942,    576,    573,   3772,\n",
      "        235265,   1417,   8563,    573,   2091,    577,   9605,    577,    573,\n",
      "          1546,   2845,   4942,    576,    573,   3772, 235265,    108, 235287,\n",
      "          5231,  41261,    798,    614,   1671,    577,   4771,    476,   8080,\n",
      "           576,  13333,  95573,   3766,   3707,   4158,   5255,  10310,    591,\n",
      "        153958,    823,   6479,  17183, 235269,   2416,  16398, 235269,    578,\n",
      "          1767,   1156,   8557,   1570,    573,   7374,    577,   9605,    577,\n",
      "          3724,   4942,    576,    573,   3772,    603,   2845, 235265,    109,\n",
      "         23081, 235269,   6137,    603,    476,  10276,   7217,    604,  19031],\n",
      "       device='cuda:0')\n",
      "\n",
      "Model output (decoded):\n",
      "user\n",
      "what is attention as described in the attention all you need paper?\n",
      "model\n",
      "In the paper \"Attention as Deep Memory\" by Vaswani et al. (2017), attention is described as a mechanism for attending to certain parts of the input sequence in order to make better predictions. This mechanism allows the model to concentrate its learning on parts of the input that are relevant to the task at hand. \n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "* **Attention is a mechanism**: It is not a single model, but rather a collection of different mechanisms within the neural network.\n",
      "* **Attention allows the model to focus on a subset of the input data** that is most relevant for the task at hand. This allows the model to perform tasks that would otherwise be impossible for a model that only considers the entire input at once.\n",
      "* **Attention gates control where the model focuses its attention**: These gates can be trained to learn specific weights for different parts of the input. This allows the model to attend to the most important parts of the input.\n",
      "* **Attention can be used to improve a variety of tasks**: These include natural language processing (NLP), machine translation, image recognition, and many other applications where the ability to attend to specific parts of the input is important.\n",
      "\n",
      "Overall, attention is a powerful tool for improving\n",
      "\n",
      "CPU times: total: 8.73 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_text = \"what is attention as described in the attention all you need paper? \"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256,\n",
    "                            do_sample=True) \n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")\n",
    "\n",
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode( outputs[0], skip_special_tokens=True)\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d2e60-22d1-4473-b866-d3d77e7a3f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e3d21e3-f00b-410b-ba70-af8f99338a69",
   "metadata": {},
   "source": [
    "#### **Augment prompts** \n",
    "We want to add relevant data to the prompt before feeding the LLM. The relevant data will come from our files by vector similarity search on the fly. meaning each time we want to query the LLM we will need to embed the query using our embedding model, bring top k similar chunks of text from our vector store, and then add this text to the prompt as context, then we prompt the LLM to get a generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "138f6919-e75b-48bb-ba93-9bab5f95f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_augmented_prompt(query, relevant_chunks):\n",
    "\n",
    "    \"\"\"\n",
    "    function to better format the prompt:\n",
    "    - use few-shot prompting (in context learning) \n",
    "    - add context from relevant chunks (augmentation)\n",
    "    \"\"\"\n",
    "    \n",
    "    # join relevant chunks in one context string\n",
    "    chunks  = [chunk[\"sentence_chunk\"] for chunk in relevant_chunks]\n",
    "    chunks = \" -\" + \"\\n -\".join(chunks)\n",
    "\n",
    "    # few-shot prompting\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query. Give yourself room to think by extracting relevant passages from the context before answering the query. Don't return the thinking, only return the answer. Make sure your answers are as explanatory as possible. Use the following examples as a reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What is the role of backpropagation in neural networks?\n",
    "Answer: Backpropagation is a key algorithm used for training neural networks by minimizing the error between predicted and actual outputs. It involves a forward pass where the input data is propagated through the network to generate an output, and a backward pass where the error is propagated back through the network to update the weights. This is done using the gradient descent optimization method, which calculates the gradient of the loss function with respect to each weight and adjusts the weights to reduce the error. Backpropagation allows neural networks to learn complex patterns in data by iteratively improving the model's accuracy.\n",
    "\\nExample 2:\n",
    "Query: How does a convolutional neural network (CNN) process image data?\n",
    "Answer:  A convolutional neural network (CNN) processes image data by applying a series of convolutional layers that automatically detect and learn features such as edges, textures, and shapes. Each convolutional layer consists of filters (also known as kernels) that slide over the input image, performing element-wise multiplication and summing the results to produce feature maps. These feature maps are then passed through activation functions (like ReLU) and pooling layers to reduce dimensionality while retaining essential features. As the data moves through deeper layers, the CNN captures increasingly abstract and complex patterns, ultimately enabling the model to recognize objects and patterns within the image. CNNs are particularly effective for tasks such as image classification, object detection, and facial recognition due to their ability to learn spatial hierarchies of features.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Add relevant chunks\n",
    "    base_prompt = base_prompt.format(context=chunks, query=query)\n",
    "    # final prompt, suited for instruction-tuned models\n",
    "    template = [{\"role\": \"user\", \"content\": base_prompt}]\n",
    "    # add_generation_prompt argument tells the template to add tokens that indicate the start of a bot response\n",
    "    prompt = tokenizer.apply_chat_template(conversation=template, tokenize=False, add_generation_prompt=True)\n",
    "   \n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d6913eb-2571-46f9-a8a4-a32fe42692ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nBased on the following context items, please answer the query. Give yourself room to think by extracting relevant passages from the context before answering the query. Don\\'t return the thinking, only return the answer. Make sure your answers are as explanatory as possible. Use the following examples as a reference for the ideal answer style.\\n\\nExample 1:\\nQuery: What is the role of backpropagation in neural networks?\\nAnswer: Backpropagation is a key algorithm used for training neural networks by minimizing the error between predicted and actual outputs. It involves a forward pass where the input data is propagated through the network to generate an output, and a backward pass where the error is propagated back through the network to update the weights. This is done using the gradient descent optimization method, which calculates the gradient of the loss function with respect to each weight and adjusts the weights to reduce the error. Backpropagation allows neural networks to learn complex patterns in data by iteratively improving the model\\'s accuracy.\\n\\nExample 2:\\nQuery: How does a convolutional neural network (CNN) process image data?\\nAnswer:  A convolutional neural network (CNN) processes image data by applying a series of convolutional layers that automatically detect and learn features such as edges, textures, and shapes. Each convolutional layer consists of filters (also known as kernels) that slide over the input image, performing element-wise multiplication and summing the results to produce feature maps. These feature maps are then passed through activation functions (like ReLU) and pooling layers to reduce dimensionality while retaining essential features. As the data moves through deeper layers, the CNN captures increasingly abstract and complex patterns, ultimately enabling the model to recognize objects and patterns within the image. CNNs are particularly effective for tasks such as image classification, object detection, and facial recognition due to their ability to learn spatial hierarchies of features.\\n\\nNow use the following context items to answer the user query:\\n -Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network.\\n -2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\\n -doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/ anthology/P19-1346. [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id= H1gx1CNKPH. [\\n -Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N dmodel dff h dk dv Pdrop ϵls train PPL BLEU params steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A) 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 (C) 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D) 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 development set, newstest2013.\\n -In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\\n -Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.\\n -Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (\\n -2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3 increased the maximum output length to input length + 300.\\n -Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\n -This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nRelevant passages: <extract relevant passages from the context here>\\nUser query: what is a transformer and how it works? what is used for? what is the compute and time complexity of it?\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is a transformer and how it works? what is used for? what is the compute and time complexity of it?\"\n",
    "\n",
    "# get relevant chunks \n",
    "scores, indices = rag_retrieve(query=query, embedding_model=embedding_model, \n",
    "                               vectore_store=embeddings, top_k=10)\n",
    "relevant_chunks = [embeddings_loaded_dict[i] for i in indices]\n",
    "\n",
    "# prepare the prompt\n",
    "prompt = prepare_augmented_prompt(query=query, relevant_chunks=relevant_chunks)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "603d28ec-a1f1-4383-a87b-64f6e1214789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is a transformer and how it works? what is used for? what is the compute and time complexity of it?\n",
      "RAG answer:\n",
      "<bos>**What is a Transformer?**\n",
      "\n",
      "A Transformer is a novel neural network architecture for sequence-to-sequence tasks. It is an extension of the self-attention mechanism, which is a powerful technique for learning long-range dependencies in sequences.\n",
      "\n",
      "**What is used for?**\n",
      "\n",
      "The Transformer is used for tasks that require learning representations of sequences of data, such as natural language processing (NLP) tasks (e.g., machine translation, text summarization, sentiment analysis).\n",
      "\n",
      "**What is the compute and time complexity?**\n",
      "\n",
      "The compute complexity of the Transformer is similar to that of a single-head self-attention layer. However, due to the use of multiple attention heads, the total computational cost is reduced. The time complexity of training a Transformer model is typically lower than that of other sequence-to-sequence models.<eos>\n",
      "CPU times: total: 7.42 s\n",
      "Wall time: 8.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# prompt the LLM\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, \n",
    "                             do_sample=True, \n",
    "                             max_new_tokens=256) \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "869c5372-ed8e-492c-a84c-eb4d025238ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add everything to one function\n",
    "def augmented_generation(query, embedding_model, vector_store, data_index,\n",
    "                   top_k, llm_model, temperature, max_new_tokens, device):\n",
    "\n",
    "    # query your RAG to get relevant text\n",
    "    scores, indices = rag_retrieve(query=query, embedding_model=embedding_model, vectore_store=vector_store, top_k=top_k)\n",
    "    relevant_chunks = [data_index[i] for i in indices]\n",
    "    \n",
    "    # prepare the prompt\n",
    "    prompt = prepare_augmented_prompt(query=query, relevant_chunks=relevant_chunks)\n",
    "\n",
    "    # prompt the LLM\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature, \n",
    "                                 do_sample=True, \n",
    "                                 max_new_tokens=max_new_tokens) \n",
    "    # decode\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # format output\n",
    "    # output_text = \n",
    "    \n",
    "    generated_response = {\"completion\": output_text,\n",
    "                          \"retrieved_chunks\": relevant_chunks}\n",
    "    return generated_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6555a5f2-3e85-4a24-87b9-ff57cff60c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Based on the following context items, please answer the query. Give yourself room to think by extracting relevant passages from the context before answering the query. Don't return the thinking, only return the answer. Make sure your answers are as explanatory as possible. Use the following examples as a reference for the ideal answer style.\n",
      "\n",
      "Example 1:\n",
      "Query: What is the role of backpropagation in neural networks?\n",
      "Answer: Backpropagation is a key algorithm used for training neural networks by minimizing the error between predicted and actual outputs. It involves a forward pass where the input data is propagated through the network to generate an output, and a backward pass where the error is propagated back through the network to update the weights. This is done using the gradient descent optimization method, which calculates the gradient of the loss function with respect to each weight and adjusts the weights to reduce the error. Backpropagation allows neural networks to learn complex patterns in data by iteratively improving the model's accuracy.\n",
      "\n",
      "Example 2:\n",
      "Query: How does a convolutional neural network (CNN) process image data?\n",
      "Answer:  A convolutional neural network (CNN) processes image data by applying a series of convolutional layers that automatically detect and learn features such as edges, textures, and shapes. Each convolutional layer consists of filters (also known as kernels) that slide over the input image, performing element-wise multiplication and summing the results to produce feature maps. These feature maps are then passed through activation functions (like ReLU) and pooling layers to reduce dimensionality while retaining essential features. As the data moves through deeper layers, the CNN captures increasingly abstract and complex patterns, ultimately enabling the model to recognize objects and patterns within the image. CNNs are particularly effective for tasks such as image classification, object detection, and facial recognition due to their ability to learn spatial hierarchies of features.\n",
      "\n",
      "Now use the following context items to answer the user query:\n",
      " -Collecting “Clean” Data Training the VPT Foundation Model via Behavioral Cloning Training the Inverse Dynamics Model (IDM) ~270k hours unlabeled video ~70k hours unlabeled video ~2k hours video labeled with actions Filter for “clean” video segments Search for relevant Minecraft videos via keywords Contractors collect data Label videos with IDM ~70k hours video IDM-labeled with actions Train non-causal IDM Train causal VPT Foundation Model a d space w a d space w Figure 2: Video Pretraining (VPT) Method Overview.3 Methods Inverse Dynamics Models (IDM) VPT, illustrated in Figure 2, requires we first collect a small amount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1... T ), which seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T observations ot : t ∈ [1... T]. In contrast to an imitation learning policy, the IDM can be non-causal, meaning its prediction for at can be a function of both past and future events, i.e. ot′>t. Compared to the behavioral cloning objective of modeling the distribution of human intent given past frames only, we hypothesize that inverting environment dynamics is easier and more data efficient to learn. Indeed, Sec.\n",
      " -Sfv: Reinforcement learning of physical skills from videos. ACM Transactions On Graphics (TOG), 37(6):1–14, 2018. [47] Feryal Behbahani, Kyriacos Shiarlis, Xi Chen, Vitaly Kurin, Sudhanshu Kasewa, Ciprian Stirbu, Joao Gomes, Supratik Paul, Frans A Oliehoek, Joao Messias, et al. Learning from demonstration in the wild. In 2019 International Conference on Robotics and Automation (ICRA), pages 775–781.\n",
      " -become weakly steerable; we believe this a rich direction for future research. Also, loss was not consistently correlated with downstream evaluation metrics (Sec.4.2), which often made progress slow and hard-won. Another fruitful future direction would be to investigate the correlation between various training metrics and downstream evaluations. Finally, while we do not anticipate any direct negative societal impacts from the models trained in this work, as VPT improves and expands to other domains it will be important to assess and mitigate harms that emerge with other forms of pretraining on internet datasets, such as emulating inappropriate behavior.67 In conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from freely available internet-scale data to sequential decision domains.\n",
      "\n",
      "Relevant passages: <extract relevant passages from the context here>\n",
      "User query: What is video pre-training (VPT)? and how they trained the VPT?\n",
      "Answer:\n",
      "model\n",
      "Sure, here's the answer to the user's query:\n",
      "\n",
      "**Video pre-training (VPT)** is a method for training large and general-purpose behavioral priors from freely available internet-scale data to sequential decision domains. This method extends the paradigm of training such priors from freely available internet-scale data to sequential decision domains.\n",
      "\n",
      "The passage also mentions the following details about VPT:\n",
      "\n",
      "- VPT involves collecting and labeling video data with accompanying actions, contrasting it with the inverse dynamics model (IDM), which requires labeled data for training.\n",
      "- The method is non-causal, meaning its prediction for at can be a function of both past and future events.\n",
      "- It is particularly effective for tasks involving image classification, object detection, and facial recognition.\n"
     ]
    }
   ],
   "source": [
    "# test pipeline\n",
    "query = \"What is video pre-training (VPT)? and how they trained the VPT?\"\n",
    "response = augmented_generation(query, embedding_model, embeddings, embeddings_loaded_dict,\n",
    "                   3, llm_model, 0.7, 512, DEVICE)\n",
    "print(response[\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d960f-82dc-40d0-ab9e-13583714cf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550f80b-19ff-42ef-a8ac-b5cec0956dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
