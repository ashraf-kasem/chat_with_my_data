{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5d95d7-b1da-4133-8162-6a1db5f43cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name\n",
    "\n",
    "\n",
    "# We instantiate the Textbox class\n",
    "textbox = gr.Textbox(label=\"Type your name here:\", placeholder=\"John Doe\", lines=2)\n",
    "\n",
    "gr.Interface(fn=greet, inputs=textbox, outputs=\"text\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54a6234-bcc3-46ba-879b-d4f85879e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2558025510ab4d929115b4938c8b4a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import run_rag as rag\n",
    "import torch\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "import gradio as gr\n",
    "\n",
    "# global variables\n",
    "VECTOR_STORE_PATH = \"vector_store/embeddings.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "NUM_OF_RELEVANT_CHUNKS = 5\n",
    "LLM_MODEL_ID = \"google/gemma-2b-it\"\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "# load the vector-store\n",
    "embeddings, data_index = rag.load_vector_store(VECTOR_STORE_PATH, DEVICE)\n",
    "\n",
    "# load the embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL,\n",
    "                                      device=DEVICE)\n",
    "\n",
    "# load LLM locally\n",
    "tokenizer, llm_model = rag.load_llm(model_id=LLM_MODEL_ID)\n",
    "\n",
    "\n",
    "def rag_answer(query):\n",
    "    response = rag.augmented_generation(query=query, embedding_model=embedding_model,\n",
    "                                        vector_store=embeddings, data_index=data_index,\n",
    "                                        top_k=NUM_OF_RELEVANT_CHUNKS, llm_model=llm_model, tokenizer=tokenizer,\n",
    "                                        temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS, device=DEVICE)\n",
    "    return response[\"completion\"]\n",
    "\n",
    "\n",
    "gr.Interface(fn=rag_answer, inputs=\"text\", outputs=\"text\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9235c80-f55a-4920-8b44-c7b193ae1b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashraf\\PycharmProjects\\chat_with_my_data\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4e38ec4f68416b82036af5fd4c5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TextStreamer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "\"\"\" This script is to start RAG pipeline \"\"\"\n",
    "\n",
    "# global variables\n",
    "VECTOR_STORE_PATH = \"vector_store/embeddings.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "NUM_OF_RELEVANT_CHUNKS = 5\n",
    "LLM_MODEL_ID = \"google/gemma-2b-it\"\n",
    "TEMPERATURE = 0.5\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "def load_vector_store(vector_store_path, device):\n",
    "    loaded_df = pd.read_csv(vector_store_path)\n",
    "    # Convert embedding column back to np.array if they were string\n",
    "    if isinstance(loaded_df[\"embedding\"][0], str):\n",
    "        loaded_df[\"embedding\"] = loaded_df[\"embedding\"].apply(\n",
    "            lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "    # Convert texts and embedding df to list of dicts (data index)\n",
    "    data_index = loaded_df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Convert embeddings to torch tensor and send to device\n",
    "    embeddings = torch.tensor(np.array(loaded_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "    return embeddings, data_index\n",
    "\n",
    "\n",
    "def rag_retrieve(query, embedding_model, vectore_store, top_k):\n",
    "    # embedd the query\n",
    "    embedded_query = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # dot product (cosine similarity because vectors are normalized)\n",
    "    scores = util.dot_score(a=embedded_query, b=vectore_store)[0]\n",
    "    # get the top k results\n",
    "    scores, indices = torch.topk(input=scores, k=top_k)\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "def show_retrieval_results(data_dict, query, scores, indices):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\\n\")\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print file path the page number\n",
    "        print(f\"File path: {data_dict[index]['file_path']}\")\n",
    "        print(f\"Page number: {data_dict[index]['page_number']}\")\n",
    "        # Print relevant sentence chunk\n",
    "        print(\"Text:\")\n",
    "        print(data_dict[index][\"sentence_chunk\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def load_llm(model_id):\n",
    "    # My GPU is Nvidia RTX 3060 with 6GB memory\n",
    "    # Loading 2 Billion  parameters model in full precision needs 2b * 4 ~ 8GB of GPU memory\n",
    "    # I need to do quantization to int-8 or int-4\n",
    "    # load in 4bit precision (boost the inference time significantly)\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                     torch_dtype=torch.bfloat16,\n",
    "                                                     quantization_config=quantization_config,\n",
    "                                                     low_cpu_mem_usage=False)\n",
    "    return tokenizer, llm_model\n",
    "\n",
    "\n",
    "def prepare_augmented_prompt(query, relevant_chunks, tokenizer):\n",
    "    \"\"\"\n",
    "    function to better format the prompt:\n",
    "    - use few-shot prompting (in context learning)\n",
    "    - add context from relevant chunks (augmentation)\n",
    "    \"\"\"\n",
    "\n",
    "    # join relevant chunks in one context string\n",
    "    chunks = [chunk[\"sentence_chunk\"] for chunk in relevant_chunks]\n",
    "    chunks = \" -\" + \"\\n -\".join(chunks)\n",
    "\n",
    "    # few-shot prompting\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query. Give yourself room to think by extracting relevant passages from the context before answering the query. Don't return the thinking, only return the answer. Make sure your answers are as explanatory as possible. Use the following examples as a reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What is the role of backpropagation in neural networks?\n",
    "Answer: Backpropagation is a key algorithm used for training neural networks by minimizing the error between predicted and actual outputs. It involves a forward pass where the input data is propagated through the network to generate an output, and a backward pass where the error is propagated back through the network to update the weights. This is done using the gradient descent optimization method, which calculates the gradient of the loss function with respect to each weight and adjusts the weights to reduce the error. Backpropagation allows neural networks to learn complex patterns in data by iteratively improving the model's accuracy.\n",
    "\\nExample 2:\n",
    "Query: How does a convolutional neural network (CNN) process image data?\n",
    "Answer:  A convolutional neural network (CNN) processes image data by applying a series of convolutional layers that automatically detect and learn features such as edges, textures, and shapes. Each convolutional layer consists of filters (also known as kernels) that slide over the input image, performing element-wise multiplication and summing the results to produce feature maps. These feature maps are then passed through activation functions (like ReLU) and pooling layers to reduce dimensionality while retaining essential features. As the data moves through deeper layers, the CNN captures increasingly abstract and complex patterns, ultimately enabling the model to recognize objects and patterns within the image. CNNs are particularly effective for tasks such as image classification, object detection, and facial recognition due to their ability to learn spatial hierarchies of features.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "\\nUser query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Add relevant chunks\n",
    "    base_prompt = base_prompt.format(context=chunks, query=query)\n",
    "    # final prompt, suited for instruction-tuned models\n",
    "    template = [{\"role\": \"user\", \"content\": base_prompt}]\n",
    "    # add_generation_prompt argument tells the template to add tokens that indicate the start of a bot response\n",
    "    prompt = tokenizer.apply_chat_template(conversation=template, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def augmented_generation(query, embedding_model, vector_store, data_index,\n",
    "                         top_k, llm_model, tokenizer, temperature, max_new_tokens, device):\n",
    "    # query your RAG to get relevant text\n",
    "    scores, indices = rag_retrieve(query=query, embedding_model=embedding_model, vectore_store=vector_store,\n",
    "                                   top_k=top_k)\n",
    "    relevant_chunks = [data_index[i] for i in indices]\n",
    "\n",
    "    # prepare the prompt\n",
    "    prompt = prepare_augmented_prompt(query=query, relevant_chunks=relevant_chunks, tokenizer=tokenizer)\n",
    "\n",
    "    # prompt the LLM\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # for streaming the response\n",
    "    response_streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    generation_kwargs = dict(**input_ids, streamer= response_streamer,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    thread = Thread(target=llm_model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "        \n",
    "    # _ = llm_model.generate(**input_ids, streamer= response_streamer,\n",
    "    #                              temperature=temperature,\n",
    "    #                              do_sample=True,\n",
    "    #                              max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # for new_text in streamer:\n",
    "    #     print(new_text, end=\"\") \n",
    "    # # decode\n",
    "    # output_text = tokenizer.decode(outputs[0])\n",
    "    # # format output\n",
    "    # output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "    \n",
    "    return response_streamer, relevant_chunks\n",
    "\n",
    "# load the vector-store\n",
    "embeddings, data_index = load_vector_store(VECTOR_STORE_PATH, DEVICE)\n",
    "\n",
    "# load the embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL,\n",
    "                                      device=DEVICE)\n",
    "\n",
    "# load LLM locally\n",
    "tokenizer, llm_model = load_llm(model_id=LLM_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe807710-7448-4eb3-97c7-b0f3ca35352d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b071f1-153c-4f06-ad5d-ce15fe3dc4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passage does not provide an explanation of what RAG is, so I cannot answer this query from the provided context.\n"
     ]
    }
   ],
   "source": [
    "query = \"explain in details the RAG?\"\n",
    "streamer, retrieved_chunks = augmented_generation(query=query, embedding_model=embedding_model,\n",
    "                                vector_store=embeddings, data_index=data_index,\n",
    "                                top_k=NUM_OF_RELEVANT_CHUNKS, llm_model=llm_model, tokenizer=tokenizer,\n",
    "                                temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d726ad-3ec0-422f-ad76-90a137cef7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "630b9cc1-3176-4f5d-8e56-6ff73a7251bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_answer(query):\n",
    "    streamer, retrieved_chunks = augmented_generation(query=query, embedding_model=embedding_model,\n",
    "                                vector_store=embeddings, data_index=data_index,\n",
    "                                top_k=NUM_OF_RELEVANT_CHUNKS, llm_model=llm_model, tokenizer=tokenizer,\n",
    "                                temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS, device=DEVICE)\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        yield generated_text\n",
    "\n",
    "\n",
    "gr.Interface(fn=rag_answer, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34109d4b-a4cb-4cd6-bfe7-d49fdfe1f966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
